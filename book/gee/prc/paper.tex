
\def\sx#1{}
\def\bx#1{#1}
\def\eq{\quad =\quad}

\title{Preconditioning}
\author{Jon Claerbout}
\maketitle

\label{paper:prc}
\sx{precondition}

%When problems have less than about $10^3$ unknowns,
%``exact'' methods are generally satisfactory.
%When there are more unknowns,
%we use iterative methods.
%Iterative methods are about as fast as exact methods,
%but they allow us to hope that when we give up exhausted
%that we are near the best that can be done.
%With more experience,
%I learned that we often remain far from the solution we seek.
%Thus we seek generally applicable problem reformulations
%that will converge more rapidly.
%Luckily we find that the helix gives a huge speedup in many problems,
%in those problems that are densely sampled in model space.


When I first realized that practical imaging methods in widespread
industrial use amounted merely to the adjoint of forward modeling,
I (and others) thought an easy way to achieve fame and fortune
would be to introduce the first steps towards inversion
along the lines of Chapter \ref{paper:lsq}.
Although inversion generally requires a prohibitive number
of steps, I felt that moving in the gradient direction,
the direction of steepest descent, would move us rapidly
in the direction of practical improvements.
This turned out to be optimistic.
It was too slow.
But then I learned about the conjugate gradient method that
spectacularly overcomes a well-known speed problem with the
method of steepest descents.
I came to realize that it was still too slow.
I learned this by watching the convergence in Figure
\ref{fig:conv}.
This led me to the helix method in Chapter \ref{paper:hlx}.
Here we'll see how it speeds many applications.

\par
We'll also come to understand why the gradient is such a poor direction
both for steepest descent and for conjugate gradients.
An indication of our path is found in the contrast between
and exact solution
$\bold m = (\bold A'\bold A)^{-1}\bold A'\bold d$ and the
gradient
$\Delta \bold m = \bold A'\bold d$
(which is the first step starting from $\bold m =\bold 0$).
Notice that $\Delta \bold m$ differs from $\bold m$
by the factor $(\bold A'\bold A)^{-1}$.
This factor is sometimes called a spectrum
and in some situations it literally is a frequency spectrum.
In these cases, $\Delta \bold m$ simply gets a different
spectrum from $\bold m$ and many iterations are required to fix it.
Here we'll find that for many problems,
``preconditioning'' with the helix is a better way.



\section{PRECONDITIONED DATA FITTING}
\par
Iterative methods (like conjugate-directions) can sometimes be accelerated
by a \bx{change of variables}.
The simplest change of variable is called a ``trial solution''.
Formally, we write the solution as
\begin{equation}
\bold m \eq \bold S \bold p
\end{equation}
where $\bold m$ is the map we seek,
columns of the matrix $\bold S$ are ``shapes'' that we like,
and coefficients in $\bold p$ are unknown coefficients
to select amounts of the favored shapes.
The variables $\bold p$  are often called the ``preconditioned variables''.
It is not necessary that $\bold S$ be an invertible matrix,
but we'll see later that invertibility is helpful.
Take this trial solution and insert it into
a typical fitting goal
\begin{equation}
\bold 0 \quad\approx\quad  \bold F \bold m \ -\  \bold d
\end{equation}
and get
\begin{equation}
\bold 0 \quad\approx\quad  \bold F \bold S \bold p \ -\  \bold d
\end{equation}
We pass the operator $\bold F \bold S$ to our iterative solver.
After finding the best fitting                                      $\bold p$,
we merely evaluate
                                                $ \bold m = \bold S \bold p$
to get the solution to the original problem.

\par
We hope this change of variables has saved effort.
For each iteration, there is a little more work:
Instead of the iterative application of
                                                $\bold F$ and $\bold F'$
we have iterative application of
                                        $\bold F\bold S$ and $\bold S'\bold F'$.

Our hope is that the number of iterations decreases because we are clever,
or because we have been lucky in our choice of $\bold S$.
Hopefully,
the extra work of the preconditioner operator $\bold S$
is not large compared to $\bold F$.
If we should be so lucky that
$\bold S= \bold F^{-1}$,
then we get the solution immediately.
Obviously we would try any guess with
$\bold S\approx \bold F^{-1}$.
Where I have known such $\bold S$ matrices,
I have often found that convergence is accelerated,
but not by much.
Sometimes it is worth using $\bold F\bold S$ for a while in the beginning,
but later it is cheaper and faster to use only $\bold F$.
A practitioner might regard the guess of $\bold S$
as prior information,
like the guess of the initial model $\bold m_0$.

\par
For a square matrix $\bold S$,
the use of a preconditioner should not change the ultimate solution.
Taking $\bold S$ to be a tall rectangular matrix,
reduces the number of adjustable parameters,
changes the solution,
gets it quicker, but lower resolution.



\subsection{Preconditioner with a starting guess}

In many applications, for many reasons,
we have a starting guess $\bold m_0$ of the solution.
You might worry that
you could not find the starting preconditioned variable 
$\bold p_0= \bold S^{-1}\bold m_0$
because you did not know the inverse of $\bold S$.
The way to avoid this problem is to
reformulate the problem
in terms of a new variable $\tilde {\bold m}$
where
$ \bold m = \tilde {\bold m} + \bold m_0$.
Then 
$\bold 0\approx \bold F \bold m - \bold d$
becomes
$\bold 0\approx \bold F \tilde {\bold m} - (\bold d - \bold F \bold m_0)$
or
$\bold 0\approx \bold F \tilde {\bold m} - \tilde {\bold d}.$
Thus we have accomplished the goal of taking
a problem with a nonzero starting model
and converting it a problem of the same type
with a zero starting model.
Thus we do not need the inverse of $\bold S$
because the iteration starts from $\tilde {\bold m}=\bold 0$
so $\bold p_0 = \bold 0$.



\section{PRECONDITIONING THE REGULARIZATION}

%\subsection{Recasting a fitting problem in white variables}

\par
The basic formulation of a geophysical estimation problem
consists of setting up
{\em  two}
goals,
one for data fitting,
and the other for model shaping.
With two goals, preconditioning is somewhat different.
The two goals may be written as:
\begin{eqnarray}
\bold 0 &\approx& \bold F \bold m - \bold d \\
\bold 0 &\approx& \bold A \bold m
\end{eqnarray}
which defines two residuals,
a so-called ``data residual'' and a ``model residual'' that
are usually minimized by conjugate-gradient, least-squares methods.
\par
To fix ideas, let us examine a toy example.
The data and the first three rows of the matrix below
are random numbers truncated to integers.
The model roughening operator $\bold A$
is a first differencing operator times 100.

%\newslide
{\samepage
\par\noindent
\footnotesize
\begin{verbatim}
d(m)     F(m,n)                                            iter  Norm
---     ------------------------------------------------   ---- -----------
 41.    -55. -90. -24. -13. -73.  61. -27. -19.  23. -55.     1 20.00396538
 33.      8. -86.  72.  87. -41.  -3. -29.  29. -66.  50.     2 12.14780140
-58.     84. -49.  80.  44. -52. -51.   8.  86.  77.  50.     3  8.94393635
  0.    100.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4  6.04517126
  0.   -100. 100.   0.   0.   0.   0.   0.   0.   0.   0.     5  2.64737511
  0.      0.-100. 100.   0.   0.   0.   0.   0.   0.   0.     6  0.79238468
  0.      0.   0.-100. 100.   0.   0.   0.   0.   0.   0.     7  0.46083349
  0.      0.   0.   0.-100. 100.   0.   0.   0.   0.   0.     8  0.08301232
  0.      0.   0.   0.   0.-100. 100.   0.   0.   0.   0.     9  0.00542009
  0.      0.   0.   0.   0.   0.-100. 100.   0.   0.   0.    10  0.00000565
  0.      0.   0.   0.   0.   0.   0.-100. 100.   0.   0.    11  0.00000026
  0.      0.   0.   0.   0.   0.   0.   0.-100. 100.   0.    12  0.00000012
  0.      0.   0.   0.   0.   0.   0.   0.   0.-100. 100.    13  0.00000000
\end{verbatim}
}
\normalsize

\par
Notice at the tenth iteration,
the residual suddenly plunges 4 significant digits.
Since there are ten unknowns and the matrix is obviously full-rank,
conjugate-gradient theory tells us to expect
the exact solution at the tenth iteration.
This is the first miracle of conjugate gradients.
(The residual actually does not drop to zero.
What is printed in the \texttt{Norm} column
is the square root of the sum of the squares
of the residual components at the \texttt{iter}-th
iteration minus that at the last interation.)


\subsection{The second miracle of conjugate gradients}

The second miracle of conjugate gradients is exhibited below.
The data and data fitting matrix are the same,
but the model damping is simplified.

%\newslide
\par\noindent
\footnotesize
\begin{verbatim}
d(m)    F(m,n)                                            iter  Norm
---    ------------------------------------------------   ----  ----------
 41.   -55. -90. -24. -13. -73.  61. -27. -19.  23. -55.     1  3.64410686
 33.     8. -86.  72.  87. -41.  -3. -29.  29. -66.  50.     2  0.31269890
-58.    84. -49.  80.  44. -52. -51.   8.  86.  77.  50.     3 -0.00000021
  0.   100.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4 -0.00000066
  0.     0. 100.   0.   0.   0.   0.   0.   0.   0.   0.     5 -0.00000080
  0.     0.   0. 100.   0.   0.   0.   0.   0.   0.   0.     6 -0.00000065
  0.     0.   0.   0. 100.   0.   0.   0.   0.   0.   0.     7 -0.00000088
  0.     0.   0.   0.   0. 100.   0.   0.   0.   0.   0.     8 -0.00000074
  0.     0.   0.   0.   0.   0. 100.   0.   0.   0.   0.     9 -0.00000035
  0.     0.   0.   0.   0.   0.   0. 100.   0.   0.   0.    10 -0.00000037
  0.     0.   0.   0.   0.   0.   0.   0. 100.   0.   0.    11 -0.00000018
  0.     0.   0.   0.   0.   0.   0.   0.   0. 100.   0.    12  0.00000000
  0.     0.   0.   0.   0.   0.   0.   0.   0.   0. 100.    13  0.00000000
\end{verbatim}
\normalsize
\par\noindent
Even though the matrix is full-rank,
we see the residual drop about 6 decimal places after the third iteration!
This convergence behavior is well known
in the computational mathematics literature.
Despite its practical importance,
it doesn't seem to have a name or identified discoverer.
So I call it the ``second miracle.''

\par
Practitioners usually don't like
the identity operator for model-shaping.
Generally they prefer to penalize wiggliness.
For practitioners,
the lesson of the second miracle of conjugate gradients
is that we have a choice of many iterations,
or learning to transform
independent variables so that
the regularization operator becomes an identity matrix.
Basically, such a transformation reduces the iteration count
from    something about the size of the model space
to      something about the size of the data space.
Such a transformation is called preconditioning.
In practice, data is often accumulated in bins.
Then the iteration count is reduced (in principle)
to the count of full bins
and should be independent of the count of the empty bins.
This allows refining the bins, enhancing the resolution.


\par
More generally,
the model goal $\bold 0 \approx \bold A \bold m$
introduces a roughening operator like a gradient,
Laplacian
(and in chapter \ref{paper:mda}
a Prediction-Error Filter (PEF)).
Thus the model goal is usually a filter,
unlike the data-fitting goal
which involves all manner of geometry and physics.
When the model goal is a filter its inverse is also a filter.
Of course this includes multidimensional filters with a helix.


\par
The preconditioning transformation
$\bold m = \bold S \bold p$
gives us
\begin{equation}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F \bold S \bold p - \bold d \\
        \bold 0 &\approx & \bold A \bold S \bold p
        \label{eqn:invintprecond}
        \end{array}
\end{equation}
The operator $\bold A$ is a roughener while $\bold S$ is a smoother.
The choices of both $\bold A$ and $\bold S$ are somewhat subjective.
This suggests that we eliminate $\bold A$ altogether
by {\em  defining} it to be proportional to the inverse of $\bold S$,
thus $\bold A\bold S=\bold I$.
The fitting goals become
\begin{equation}
        \label{eqn:whitevar1}
        \begin{array}{lll}
        \bold 0 &\approx & \bold F  \bold S \bold p - \bold d \\
        \bold 0 &\approx & \epsilon\ \bold p
        \end{array}
\end{equation}
which enables us to benefit from the ``second miracle''.
After finding $\bold p$,
we obtain the final model with $\bold m = \bold S \bold p$.

%Because model shaping is generally a filtering operation,
%and because preconditioning operators are best when
%they are invertible,

%\par
%As before (without regularization) we need a pair of
%roughening and smoothing operators that are mutually inverse
%$ \bold A\bold S=\bold I$.
%We need the roughener $\bold A$
%to get started with
%$\bold p_0=\bold A\bold m_0$,
%and we need the smoother $\bold S$ at each iteration.
%Besides Fourier transforms,
%the only known multidimensional roughening-smoothing operator pairs
%are the family of filters on a helix.
%Thus    deconvolution on a helix is an all-purpose
%        preconditioning strategy for multidimensional model regularization.


%\begin{notforlecture}
\subsection{Importance of scaling}

Another simple toy example shows us the importance of scaling.
We use the same example as above
except that the $i$-th column is multiplied by $i/10$
which means the $i$-th model variable has been divided by $i/10$.

%\newslide
\par\noindent
\footnotesize
\begin{verbatim}
d(m)    F(m,n)                                            iter  Norm
---    ------------------------------------------------   ---- -----------
 41.    -6. -18.  -7.  -5. -36.  37. -19. -15.  21. -55.     1 11.59544849
 33.     1. -17.  22.  35. -20.  -2. -20.  23. -59.  50.     2  6.97337770
-58.     8. -10.  24.  18. -26. -31.   6.  69.  69.  50.     3  5.64414406
  0.    10.   0.   0.   0.   0.   0.   0.   0.   0.   0.     4  4.32118177
  0.     0.  20.   0.   0.   0.   0.   0.   0.   0.   0.     5  2.64755201
  0.     0.   0.  30.   0.   0.   0.   0.   0.   0.   0.     6  2.01631355
  0.     0.   0.   0.  40.   0.   0.   0.   0.   0.   0.     7  1.23219979
  0.     0.   0.   0.   0.  50.   0.   0.   0.   0.   0.     8  0.36649203
  0.     0.   0.   0.   0.   0.  60.   0.   0.   0.   0.     9  0.28528941
  0.     0.   0.   0.   0.   0.   0.  70.   0.   0.   0.    10  0.06712411
  0.     0.   0.   0.   0.   0.   0.   0.  80.   0.   0.    11  0.00374284
  0.     0.   0.   0.   0.   0.   0.   0.   0.  90.   0.    12 -0.00000040
  0.     0.   0.   0.   0.   0.   0.   0.   0.   0. 100.    13  0.00000000
\end{verbatim}
\normalsize
We observe that solving the same problem for the scaled variables
has required a severe increase
in the number of iterations required to get the solution.
We lost the benefit of the second CG miracle.
Even the rapid convergence predicted for the 10-th iteration
is delayed until the 12-th.
%\end{notforlecture}

\subsection{Statistical interpretation}
This book is not a statistics book.
Never-the-less, many of you have some statistical knowledge
that allows you a statistical interpretation
of these views of preconditioning.
\par
A statistical concept is that we can combine many streams
of random numbers into a composite model.
Each stream of random numbers is generally taken
to be uncorrelated with the others,
to have zero mean, and to have the same variance
as all the others.
This is often abbreviated as IID, denoting
Independent, Identically Distributed.
Linear combinations
like filtering and weighting operations
of these IID random streams
can build correlated random functions much like those
observed in geophysics.
A geophysical practitioner seeks to do the inverse,
to operate on the correlated unequal random variables
and create the statistical ideal random streams.
The identity matrix required for the ``second miracle'',
and our search for a good preconditioning transformation
are related ideas.
The relationship will become more clear in chapter \ref{paper:mda}
when we learn how to estimate the best roughening operator $\bold A$
as a prediction-error filter.

\par
\boxit{
        Two philosophies to find a preconditioner:
        \begin{enumerate}
        \item
        Dream up a smoothing operator $\bold S$.
        \item
        Estimate a prediction-error filter $\bold A$,
        and then use its inverse $\bold S = \bold A^{-1}$.
        \end{enumerate}
        }


\par
\boxit{
        Deconvolution on a helix is an all-purpose
        preconditioning strategy for multidimensional model regularization.
        }


\par
The outstanding acceleration of convergence by preconditioning
suggests that the philosophy of image creation by optimization
has a dual orthonormality:
First, Gauss (and common sense) tells us that the data residuals
should be roughly equal in size.  Likewise in Fourier space
they should be roughly equal in size, which means they should
be roughly white, i.e. orthonormal.
(I use the word ``orthonormal''
because white means the autocorrelation is an impulse,
which means the signal is statistically orthogonal
to shifted versions of itself.)
Second,
to speed convergence of iterative methods,
we need a whiteness, another orthonormality, in the solution.
The map image, the physical function that we seek, might
not be itself white, so we should solve first for another variable,
the whitened map image, and as a final step,
transform it to the ``natural colored'' map.


\subsection{The preconditioned solver}
Summing up the ideas above,
we start from fitting goals
\begin{equation}
\begin{array}{lll}
\label{eq:main}
\bold 0 &\approx& \bold F \bold m \ -\  \bold d \\
\bold 0 &\approx& \bold A \bold m
\end{array}
\end{equation}
and we change variables from
$\bold m$ to $\bold p$ using
$\bold m = \bold A^{-1} \bold p$
\begin{equation}
\begin{array}{llllcl}
\bold 0 &\approx &  \bold F \bold m \ -\  \bold d   &=&
    \bold F  \bold A^{-1} &\bold p  \ -\  \bold d
\\
\bold 0 &\approx &  \bold A \bold m       &=&   \bold I        & \bold p
\end{array}
\label{eqn:precsummary}
\end{equation}
Preconditioning means iteratively fitting
by adjusting the $\bold p$ variables
and then finding the model by using
$\bold m = \bold A^{-1} \bold p$.

\begin{comment}
A new reusable
preconditioned solver is
the module \texttt{solver_prc} \vpageref{lst:solver_prc}.
%The variable \texttt{x} in \texttt{prec\_solver} refers to $\bold m$.
Likewise the modeling operator $\bold F$ is called \texttt{Fop}
and the smoothing operator $\bold A^{-1}$ is called \texttt{Sop}.
Details of the code are only slightly different from
the regularized solver
\texttt{solver_reg} \vpageref{lst:solver_reg}.

%REMEMBER TO PUT THE PROGRAM BACK IN HERE !
%\begin{notforlecture}
\moddex{solver_prc}{Preconditioned solver}
%\end{notforlecture}
\end{comment}


\section{OPPORTUNITIES FOR SMART DIRECTIONS}
Recall the fitting goals (\ref{eqn:precsummary})
\begin{equation}
\begin{array}{llllllcl}
\bold 0 &\approx& \bold r_d &=&  \bold F \bold m \ -\  \bold d   &=&
    \bold F  \bold A^{-1} &\bold p  \ -\  \bold d
    \\
\bold 0 &\approx& \bold r_m &=&  \bold A \bold m       &=&
    \bold I        & \bold p
\end{array}
\label{eqn:precsummary}
\end{equation}
Without preconditioning we have the search direction
\begin{equation}
\Delta \bold m_{\rm bad} \eq
\left[
	\begin{array}{cc}
	\bold F' & \bold A'
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold r_d \\
	\bold r_m
	\end{array}
\right]
\end{equation}
and with preconditioning we have the search direction
\begin{equation}
\Delta \bold p_{\rm good} \eq
\left[
	\begin{array}{cc}
	(\bold F\bold A^{-1})' & \bold I
	\end{array}
\right]
\left[
	\begin{array}{c}
	\bold r_d \\
	\bold r_m
	\end{array}
\right]
\end{equation}
\par
The essential feature of preconditioning is not that we perform
the iterative optimization in terms of the variable $\bold p$.
The essential feature is that we use a search direction
that is a gradient with respect to $\bold p'$ not $\bold m'$.
Using $\bold A\bold m=\bold p$ we have
$\bold A\Delta \bold m=\Delta \bold p$.
This enables us to define a good search direction in model space.
\begin{equation}
\Delta \bold m_{\rm good} \eq \bold A^{-1}
\Delta \bold p_{\rm good} \eq
	\bold A^{-1} (\bold A^{-1})'
	\bold F' \bold r_d + \bold  A^{-1} \bold r_m
\end{equation}
Define the gradient by $\bold g=\bold F'\bold r_d$ and
notice that $\bold r_m=\bold p$.
\begin{equation}
\Delta \bold m_{\rm good} \eq
	\bold A^{-1} (\bold A^{-1})' \ \bold g
%	{ \bold g \over \bold A \bold A'}
	+ \bold m
\label{eqn:newdirection}
\end{equation}

\par
The search direction (\ref{eqn:newdirection}) 
shows a positive-definite operator scaling the gradient.
Each component of any gradient vector is independent of each other.
All independently point a direction for descent.
Obviously, each can be scaled by any positive number.
Now we have found that we can also scale a gradient vector by
a positive definite matrix and we can still expect
the conjugate-direction algorithm to descend, as always,
to the ``exact'' answer in a finite number of steps.
This is because modifying the search direction with
$ \bold A^{-1} (\bold A^{-1})'$ is equivalent to solving
a conjugate-gradient problem in $\bold p$.

%\subsubsection{Implications for better preconditioning}
%\par
%The search direction (\ref{eqn:newdirection})
%brings us both good news and bad news.
%First the bad news.
%Equation (\ref{eqn:newdirection}) looks like something new
%but it will probably turn out to be equivalent to all the
%preconditioning work we have done with the operator $\bold A^{-1}$
%being polynomial division.
%Actually, we do polynomial division
%followed by truncation of the infinite series.
%Because of the truncation we really do not properly construct
%$ \bold A^{-1} (\bold A^{-1})'$, and for this we can expect
%some difficulty at one boundary.

%\par
%The good news is that we now see new preconditioning opportunities.
%Formerly we began from a preconditioning 
%operator $\bold A^{-1}$ but now we see that we really only
%need its spectrum $\bold A^{-1} (\bold A^{-1})'$.
%That might not be any real advantage since our basic representation
%of a positive definite matrix is any matrix times its transpose.
%Anyway, we can conclude that the operator $\bold A$
%may be chosen as any cascade of weighting and filtering operators.
%
%\par
%We are also reminded that what we really need
%is not simply $\bold A^{-1} (\bold A^{-1})'$
%but $(\bold F'\bold F+\bold A'\bold A)^{-1}$.
%Likewise we imagine this being built of
%any cascade of weighting and filtering operators
%followed by the adjoint.



\section{NULL SPACE AND INTERVAL VELOCITY}
A bread-and-butter problem in seismology is building the velocity
as a function of depth (or vertical travel time)
starting from certain measurements.
The measurements are described elsewhere (BEI for example).
They amount to measuring the integral of the velocity squared
from the surface down to the reflector.
It is known as the RMS (root-mean-square) velocity.
Although good quality echos may arrive often,
they rarely arrive continuously for all depths.
Good information is interspersed unpredictably with poor information.
Luckily we can
also estimate
the data quality by the ``coherency'' or the
``stack energy''.
In summary, what we get from observations and preprocessing
are two functions of travel-time depth,
(1) the integrated (from the surface) squared velocity, and
(2) a measure of the quality of the integrated velocity measurement.
Some definitions:
\begin{description}
\item  [$\bold d$]
is a data vector whose components range over the vertical
traveltime depth $\tau$,
and whose component values contain the scaled RMS velocity squared
$\tau v_{\rm RMS}^2/\Delta \tau $
where
$\tau /\Delta \tau $ is the index on the time axis.
\item [$\bold W$]
is a diagonal matrix along which we lay the given measure
of data quality.  We will use it as a weighting function.
\item  [$\bold C$]
is the matrix of causal integration, a lower triangular matrix of ones.
\item  [$\bold D$]
is the matrix of causal differentiation, namely, $\bold D=\bold C^{-1}$.
\item [$\bold u$]
is a vector whose components range over the vertical
traveltime depth $\tau$,
and whose component values contain the interval velocity squared
$v_{\rm interval}^2 $.
\end{description}
From these definitions,
under the assumption of a stratified earth with horizontal reflectors
(and no multiple reflections)
the theoretical (squared) interval velocities
enable us to define the theoretical (squared) RMS velocities by
\begin{equation}
\bold C\bold u \eq \bold d 
\end{equation}
With imperfect data, our data fitting goal is to minimize the residual
\begin{equation}
\bold 0
\quad\approx\quad
\bold W
\left[
\bold C\bold u
-
\bold d
\right]
\end{equation}
\par
To find the interval velocity
where there is no data (where the stack power theoretically vanishes)
we have the ``model damping'' goal to minimize
the wiggliness $\bold p$
of the squared interval velocity $\bold u$.
\begin{equation}
\bold 0
\quad\approx\quad
\bold D \bold u \eq \bold p
\end{equation}
\par
We precondition these two goals
by changing the optimization variable from
interval velocity squared
$\bold u$ to its wiggliness $\bold p$.
Substituting $\bold u=\bold C\bold p$ gives the two goals
expressed as a function of wiggliness $\bold p$.
\begin{eqnarray}
\label{eqn:model}
\bold 0
&\approx&
\bold W
\left[
\bold C^2\bold p
-
\bold d
\right]
\\
\bold 0
&\approx&
\epsilon \; \bold p
\end{eqnarray}
\subsection{Balancing good data with bad}
\inputdir{bob}
Choosing the size of $\bold \epsilon$ chooses
the stiffness of the curve that connects regions of good data.
Our first test cases gave solutions
that we interpreted to be
too stiff at early times and too flexible at later times.
This leads to two possible ways to deal with the problem.
One way modifies the model shaping
and the other modifies the data fitting.
The program below weakens the data fitting weight with time.
This has the same effect as stiffening the model shaping with time.

\plot{clapp}{width=6in,height=3in}{
  Raw CMP gather (left),
  Semblance scan (middle),
  and semblance value used for weighting function (right).
  (Clapp)
}

\plot{stiff}{width=6in,height=3in}{
  Observed RMS velocity
  and that predicted by a stiff model with
  $\epsilon=4$.
  (Clapp)
}

\plot{flex}{width=6in,height=3in}{
  Observed RMS velocity
  and that predicted by a flexible model with
  $\epsilon=.25$
  (Clapp)
}

%\subsection{Bigsolver}
%The regression (\ref{eqn:model}) includes a weighting function,
%so we need yet another solver module
%very much like the regularized and preconditioned solvers
%that we used earlier.
%The idea is essentially the same.
%Instead of preparing such a solver here,
%I refer you to the end of the book for
%\texttt{solver\_mod} \vpageref{lst:bigsolver},
%a big solver that incorporates everything
%that we need in the book.
%Hopefully we will not need to look at solvers again for a while.
%Module \texttt{vrms2int} \vpageref{lst:vrms2int}
%was written by Bob Clapp to get the results
%in Figures \ref{fig:clapp} to \ref{fig:flex}.
%Notice that he is using
%\texttt{solver\_mod} \vpageref{lst:bigsolver}.

\begin{comment}
\moddex{vrms2int}{Converting RMS to interval velocity}
\end{comment}

\subsection{Lateral variations}
The analysis above appears one dimensional in depth.
Conventional interval velocity estimation builds a velocity-depth model
independently at each lateral location.
Here we have a logical path for combining measurements
from various lateral locations.
We can change the regularization
to something like $\bold 0\approx \nabla\bold u$.
Instead of merely minimizing the vertical gradient of velocity
we minimize its spatial gradient.
Luckily we have preconditioning and the helix to speed the solution.
\par
%Likewise the construction of the data quality screen $\bold G$
%would naturally involve the full three-dimensional setting.

\subsection{Blocky models}
\inputdir{.}

Sometimes we seek a velocity model that increases smoothly
with depth through our scattered
measurements of good-quality RMS velocities.
Other times, we seek a blocky model.
(Where seismic data is poor,
a well log could tell us whether to choose smooth or blocky.)
Here we see an estimation method that can choose the blocky alternative,
or some combination of smooth and blocky.

\par
Consider the five layer model in Figure \ref{fig:rosales}.
Each layer has unit traveltime thickness
(so integration is simply summation).
Let the squared interval velocities be $(a,b,c,d,e)$
with strong reliable reflections at the base of layer $c$ and layer $e$,
and weak, incoherent, ``bad'' reflections at bases of $(a,b,d)$.
Thus we measure $V_c^2$ the RMS velocity squared of the top three layers
and $V_e^2$ that for all five layers.
Since we have no reflection from at the base of the fourth layer,
the velocity in the fourth layer is not measured but a matter for choice.
In a smooth linear fit we would want $d=(c+e)/2$.
In a blocky fit we would want $d=e$.

\sideplot{rosales}{width=1.5in}{
  A layered earth model.
  The layer interfaces cause reflections.
  Each layer has a constant velocity in its interior.
}


\par
Our screen for good reflections looks like $(0,0,1,0,1)$
and our screen for bad ones looks like the complement $(1,1,0,1,0)$.
We put these screens on the diagonals of diagonal matrices
$\bold G$ and $\bold B$.
Our fitting goals are:
\begin{eqnarray}
3V_c^2 &\approx& a+b+c
\\
5V_e^2 &\approx& a+b+c+d+e
\\
u_0 &\approx& a
\\
0 &\approx& -a+b
\\
0 &\approx& -b+c
\\
0 &\approx& -c+d
\label{eqn:block}
\\
0 &\approx& -d+e
\end{eqnarray}
For the blocky solution, we do not want the fitting goal (\ref{eqn:block}).
Further explanations await completion of examples.

%We can remove it by multiplying the model goals by a diagonal-matrix
%badpass screen $\bold B$ 
%eliminating the goal $0 \approx -c+d$.
%In abstract, our fitting goals become
%\begin{eqnarray}
%\bold 0 &\approx& \bold r \eq \bold C \bold u  - \bold d
%\\
%\bold 0 &\approx& \bold p \eq \bold B\bold D\bold u - \bold u_0
%\end{eqnarray}
%where $\bold u_0$ is a zero vector with a top component of $u_0$.
%Since $\bold B$ is not invertable,
%we cannot backsolve the preconditioned variable $\bold p$ for
%the squared interval velocity
%$\bold u= \bold D^{-1}\bold B^{-1}(\bold p + \bold u_0)$.
%Instead, we use $\bold G$ for $\bold B^{-1}$
%thus redefining
%the implicit relationship for $\bold u$.
%\begin{equation}
%\bold u \eq \bold u_0 + \bold D^{-1}\bold G\bold p
%\label{eqn:ufromp}
%\end{equation}
%where $\bold G$ is the goodpass screen.
%Since $\bold D^{-1}=\bold C$ the fitting goals become
%
%
%\begin{equation}
%\begin{array}{lll}
%\bold 0 &\approx& \bold r \eq \bold C^2\bold G \bold p
%+\bold C \bold u_0 
%-\bold d
%\\
%\bold 0 &\approx& \bold p
%\end{array}
%\label{eqn:logical}
%\end{equation}
%After fitting with $\bold p$,
%we define the squared interval-velocity
%$\bold u$ using (\ref{eqn:ufromp}).
%
%\par
%The formulation
%(\ref{eqn:logical})
%is so logical that we might have guessed it:
%The goal $\bold 0 \approx \bold p $ says that $\bold p$ is mostly zero.
%What emerges from $\bold G$ is a sprinkling of impulses.
%Then $\bold C^2$ converts the pulses to ramp functions
%(zero until a certain place, then growing linearly)
%which are used to fit the data (integrated velocity).
%Differentiating the data-fitting ramps
%converts them to the desired blocks of constant velocity.
%One iteration is required for each impulse.
%
%\par
%Choosing $\bold G$ to be an identity $\bold I$ gives smooth velocity models,
%such as caused by the increasing consolidation of the rocks with depth.
%Choosing the screen $\bold G$ to have a sprinkling of pass locations
%picks the boundaries of blocks of constant velocity.
%The choice can be made by people with subjective criteria (like geologists)
%or we can assist by using the data itself
%in various ways to select our degree of preference between
%the blocky and smooth models.
%For example,
%we could put seismic coherency or amplitude
%on the goodpass diagonal matrix $\bold G$.
%Clearly, much remains to be gained from experience.
%



%\begin{notforlecture}



\section{INVERSE LINEAR INTERPOLATION}
\inputdir{sep94}
\sideplot{data}{width=3in,height=1.5in}{
  The input data are irregularly sampled.
}
The first example is a simple synthetic test for 1-D inverse
interpolation. The input data were randomly subsampled (with
decreasing density) from a sinusoid (Figure \ref{fig:data}). The
forward operator $\bold L$ in this case is linear interpolation. We seek
a regularly sampled model that could predict the data with a
forward linear interpolation. Sparse irregular distribution of the
input data makes the regularization enforcement a necessity.
I applied convolution with the simple $(1,-1)$
difference filter as the operator $\bold D$ that forces model continuity
(the first-order spline).
An appropriate preconditioner $\bold S$ in this
case is recursive causal integration. 

%Figures \ref{fig:im1} and
%\ref{fig:fm1} show the results of inverse interpolation after
%exhaustive 300 iterations of the conjugate-direction method.
%As a result of using the causal integration for preconditioning,
%the rightmost part of the model in the data-space case stays at a
%constant level instead of decreasing to zero. If we specifically
%wanted a zero-value boundary condition, it wouldn't be difficult to
%implement it by adding a zero-value data point at the boundary.

%\sideplot{im1}{width=3in,height=1.5in}{ Estimation of a continuous
%  function b regularization. The regularization operator $\bold A$ is
%  the derivative operator (convolution with $(1,-1)$).}

%\sideplot{fm1}{width=3in,height=1.5in}{Estimation of a continuous
%  function by preconditioning model regularization. The
%  preconditioning operator $\bold P$ is causal integration.}

\plot{conv}{width=6in,height=7in}{Convergence history of inverse
  linear interpolation. Left: regularization, right: preconditioning.
  The regularization operator $\bold A$ is
  the derivative operator (convolution with $(1,-1)$. The
  preconditioning operator $\bold S$ is causal integration.}

%\plot{conv2}{width=6in,height=7in}{Convergence history of inverse
%  linear interpolation. Left: regularization, right: preconditioning.
%  The regularization operator $\bold A$ is
%  the second derivative operator (convolution with $(1,-2,1)$. The
%  preconditioning operator $\bold P$ is the corresponding inverse filtering.}

As expected,
preconditioning provides a much faster rate of convergence.
Since iteration to the exact solution
is never achieved in large-scale problems,
the results of iterative optimization may turn out quite differently.
Bill Harlan points out that the two goals
in (\ref{eq:main}) conflict with each other:
the first one enforces ``details'' in the model,
while the second one tries to smooth them out.
Typically, regularized optimization creates
a complicated model at early iterations.
At first, the data fitting goal (\ref{eq:main}) plays a more important role.
Later, the regularization goal (\ref{eq:main}) comes into play
and simplifies (smooths) the model as much as needed.
Preconditioning acts differently.
The very first iterations create a simplified (smooth) model.
Later, the data fitting goal adds more details into the model.
If we stop the iterative process early,
we end up with an insufficiently complex model,
not in an insufficiently simplified one.
Figure \ref{fig:conv} provides a clear illustration of Harlan's observation.

\par
Figure \ref{fig:schwab1}
measures the rate of convergence by the model residual,
which is a distance from the current model to the final solution.
It shows that preconditioning saves many iterations.
Since the cost of each iteration for each method is roughly equal,
the efficiency of preconditioning is evident.
\sideplot{schwab1}{width=2.4in}{
  Convergence of the iterative optimization,
  measured in terms of the model residual.
  The ``p'' points stand for preconditioning;
  the ``r'' points,
  regularization.
}

\begin{comment}
\par
The module \texttt{invint2} \vpageref{lst:invint2}
invokes the solvers to make
Figures \ref{fig:conv}
and
\ref{fig:schwab1}.
We use convolution with
\texttt{helicon} \vpageref{lst:helicon}
for the regularization
and we use deconvolution with
\texttt{polydiv} \vpageref{lst:polydiv}
for the preconditioning.
The code looks fairly straightforward except for
the oxymoron
\texttt{known=aa\%mis}.

\moddex{invint2}{Inverse linear interpolation}        
\end{comment}

%\par
%This example suggests that the philosophy of image creation by
%optimization has a dual orthonormality: First, Gauss (and common
%sense) tells us that the data residuals should be roughly equal in
%size.  Likewise in Fourier space they should be roughly equal in size,
%which means they should be roughly white, i.e. orthonormal.  (I use
%the word ``orthonormal'' because white means the autocorrelation is an
%impulse, which means the signal is statistically orthogonal to shifted
%versions of itself.)  Second, to speed convergence of iterative
%methods, we need a whiteness, another othonormality, in the solution.
%The map image, the physical function that we seek, might not be itself
%white, so we should solve first for another variable, the whitened map
%image, and as a final step, transform it to the ``natural colored''
%map.
%
%\par
%Often geophysicists create a preconditioning matrix $\bold B$ by
%inventing columns that ``look like'' the solutions that they seek.
%Then the space $\bold x$ has many fewer components than the space of
%$\bold m$.  This approach is touted as a way of introducing geological
%and geophysical prior information into the solution.  Indeed, it
%strongly imposes the form of the solution.  Perhaps this approach
%deserves the diminutive term ``curve fitting'' instead of the
%grandiloquent ``geophysical inverse theory.''  Our preferred approach
%is not to invent the columns of the preconditioning matrix, but to
%estimate the prediction-error filter of the model and use its inverse.
%



\section{EMPTY BINS AND PRECONDITIONING}
There are at least three ways to fill empty bins.
Two require a roughening operator $\bold A$ while
the third requires a smoothing operator which
(for comparison purposes) we denote $\bold A^{-1}$.
The three methods are generally equivalent
though they differ in important details.

\par
The original way in
Chapter \ref{paper:iin} is to
restore missing data
by ensuring that the restored data,
after specified filtering,
has minimum energy, say
$\bold A\bold m\approx \bold 0$.
Introduce the selection mask operator $\bold K$, 
a diagonal matrix with
ones on the known data and zeros elsewhere
(on the missing data).
Thus
$ \bold 0 \approx \bold A(\bold I-\bold K+\bold K)\bold m $ or
\begin{equation}
\bold 0 \quad\approx\quad
\bold A (\bold I-\bold K) \bold m
\ +\ 
\bold A \bold m_k\;,
\label{eqn:style0}
\end{equation}
where we define $\bold m_k$ to be the data
with missing values set to zero by 
$\bold m_k=\bold K\bold m$.

\par
A second way to find missing data is with the set of goals
\begin{equation}
\begin{array}{rrrrr}
\bold 0 & \approx &          \bold K \bold m  & - & \bold m_k \\
\bold 0 & \approx & \epsilon \bold A \bold m  &   &
\end{array}
\label{eqn:style1}
\end{equation}
and take the limit as the scalar $\epsilon \rightarrow 0$.
At that limit, we should have the same result
as equation (\ref{eqn:style0}).

\par
There is an important philosophical difference between
the first method and the second.
The first method strictly honors the known data.
The second method acknowledges that when data misfits
the regularization theory, it might be the fault of the data
so the data need not be strictly honored.
Just what balance is proper falls to the numerical choice of $\epsilon$,
a nontrivial topic.

\par
A third way to find missing data is to precondition
equation (\ref{eqn:style1}),
namely, try the substitution 
$\bold m = \bold A^{-1} \bold p $.
\begin{equation}
\begin{array}{rrrrr}
\bold 0 & \approx & \bold K \bold A^{-1} \bold p  &-& \bold m_k \\
\bold 0 & \approx & \epsilon             \bold p  & &
\end{array}
\label{eqn:style2}
\end{equation}
There is no simple way of knowing beforehand
what is the best value of $\epsilon$.
Practitioners like to see solutions for various values of $\epsilon$.
Of course that can cost a lot of computational effort.
Practical exploratory data analysis is more pragmatic.
Without a simple clear theoretical basis,
analysts generally begin from $\bold p=0$
and abandon the fitting goal $\epsilon \bold I \bold p\approx 0$.
Implicitly, they take $\epsilon=0$.
Then they examine the solution as a function of iteration,
imagining that the solution at larger iterations
corresponds to smaller $\epsilon$.
There is an eigenvector analysis
indicating some kind of basis for this approach,
but I believe there is no firm guidance.

\par
Before we look at coding details for the three methods
of filling the empty bins,
we'll compare results of trying all three methods.
For the roughening operator $\bold A$,
we'll take the helix derivative $\bold H$.
This is logically equivalent to roughening with the gradient $\bold \nabla$
because the (negative) laplacian operator is
$\bold\nabla'\bold\nabla = \bold H'\bold H $.

%Later, in Chapter
%\ref{mda/paper:mda}
%we'll find a smarter roughening operator $\bold A$ that gives better results.


\subsection{SEABEAM: Filling the empty bins with a laplacian}
\sx{seabeam}
\inputdir{seab}

Figure~\ref{fig:seabin} shows a day's worth of data\footnote{
        I'd like to thank Alistair Harding
        for this interesting data set
        named April 18.
        \sx{Harding, Alistair}
        }
collected at sea by \bx{SeaBeam},
an apparatus for measuring water depth both
directly under a ship, and somewhat off to the sides of the ship's track.
The data is measurements of depth $h(x,y)$ at miscellaneous locations
in the $(x,y)$-plane.
\sideplot{seabin}{width=3.0in,height=3.0in}{
  Depth of the ocean under ship tracks.
  Empty bins are displayed with an average depth $\bar h$.
}
The locations are scattered about,
according to various aspects
of the ship's navigation and the geometry of the SeaBeam sonic antenna.
Figure \ref{fig:seabin} was made by binning with
\texttt{bin2()} \vpageref{lst:bin2}
and equation (\ref{eqn:dpbin}).
The spatial spectra of the noise in the data
could be estimated where tracks cross over themselves.
This might be worth while, but we do not pursue it now.

\inputdir{seabeam}

\par
Here we focus on the empty mesh locations where no data is recorded
(displayed with the value of the mean depth $\bar h$).
These empty bins were filled with
module \texttt{mis2} \vpageref{lst:mis2}.
Results are in Figure \ref{fig:prcfill}.
\plot{prcfill}{width=6in,height=8in}{
  The ocean bottom restoring missing data with a helix derivative.
}
In Figure \ref{fig:prcfill} the left column results from 20 iterations
while the right column results from 100 iterations.
\par
The top row in Figure \ref{fig:prcfill} shows that more iterations
spreads information further into the region of missing data.
\par
It turned out that the original method strictly honoring known data
gave results so similar to the second method (regularizing)
that the plots could not be visually distinguished.
The middle row in Figure \ref{fig:prcfill} therefore shows
the difference in the result of the two methods.
We see an outline of the transition between known and unknown regions.
Obviously, the missing data is pulling known data towards zero.

\par
The bottom row in Figure \ref{fig:prcfill} shows that preconditioning
spreads information to great distances much quicker
but early iterations make little effort to honor the data.
(Even though these results are for $\epsilon=0$.)
Later iterations make little change at long distance
but begin to restore sharp details on the small features
of the known topography.

\par
What if we can only afford 100 iterations?
Perhaps we should first do 50 iterations with
preconditioning to develop the remote part of the solution and
then do 50 iterations by one of the other two methods to be sure
we attended to the details near the known data.
A more unified approach (not yet tried, as far as I know)
would be to unify the techniques.
The conjugate direction method searches two directions,
the gradient and the previous step.
We could add a third direction,
the smart direction of equation (\ref{eqn:newdirection}).
Instead of having a $2\times 2$ matrix
solution like equation (\ref{eqn:twobytwosln}) for two distances,
we would need to solve a $3\times 3$ matrix for three.

\par
Figure \ref{fig:prcfill} has a few artifacts connected with the
use of the helix derivative.
Examine equation (\ref{eqn:lapfac}) to notice the shape of the helix derivative.
In principle, it is infinitely long in the horizontal axis
in both equation (\ref{eqn:lapfac}) and Figure \ref{fig:prcfill}.
In practice, it is truncated.  The truncation is visible as bands
along the sides of Figure \ref{fig:prcfill}.

\par
As a practical matter, no one would use the first two bin filling methods
with helix derivative for the roughener
because it is theoretically equivalent to the gradient operator
$\bold \nabla$ which has many fewer coefficients.
Later, in Chapter
\ref{paper:mda}
we'll find a much smarter roughening operator $\bold A$
called the Prediction Error Filter (PEF)
which gives better results.





%The top row in Figure \ref{fig:prcfill} shows that more iterations
%The second 
%After many iterations, both regularization and preconditioning
%lead us to the same result.
%After a small number of iterations, we see that
%regularization has filled the small holes
%but it has not reached out far away from the known data.
%With preconditioning, it is the opposite.


\subsection{Three codes for inverse masking}
The selection (or masking) operator $\bold K$ is
implemented in
\texttt{mask()} \vpageref{lst:mask}.
\opdex{mask}{copy under mask}{45}{50}{filt/lib}
%The inverting of the mask operator proceeds much as
%we inverted the linear interpolation operator with
%module \texttt{invint2} \vpageref{lst:invint2}.
%The main difference is we swap the selection operator
%for the linear interpolation operator.
%(Philosophically, selection is like binning which is
%like nearest-neighbor interpolation.)

All the results shown in Figure \ref{fig:prcfill}
were created with the module
\texttt{mis2} \vpageref{lst:mis2}.
Code locations with \texttt{style=0,1,2}
correspond to the fitting goals
(\ref{eqn:style0}), (\ref{eqn:style1}), (\ref{eqn:style2}).
\moddex{mis2}{Missing data interpolation with and without preconditioning}{25}{55}{user/gee} %XX
\par

%It is instructive to compare
%\texttt{mis2} \vpageref{lst:mis2} with
%\texttt{invint2} \vpageref{lst:invint2}.
%Both are essentially filling empty regions
%consistant with prior knowledge at particular locations
%and minimizing energy of the filtered field.
%Both use the helix and can be used in $N$-dimensional space.


\section{THEORY OF UNDERDETERMINED LEAST-SQUARES}
Construct theoretical data with
\begin{equation}
         \bold d \eq \bold F \bold m
        \label{eqn:theordata}
\end{equation}
Assume there are fewer data points than model points
and that the matrix $\bold F \bold F'$ is invertible.
From the theoretical data we estimate a model $\bold m_0$ with
\begin{equation}
         \bold m_0 \eq \bold F' (\bold F \bold F')^{-1} \bold d
        \label{eqn:underdetest}
\end{equation}
To verify the validity of the estimate,
insert the estimate (\ref{eqn:underdetest}) into the
data modeling equation (\ref{eqn:theordata}) and notice
that the estimate $\bold m_0$ predicts the correct data.
Notice that equation
(\ref{eqn:underdetest}) is not the same
as equation (\ref{eqn:sln}) which we derived much earlier.
What's the difference?
The central issue is which matrix of
$\bold F \bold F'$ and
$\bold F' \bold F$ actually has an inverse.
If $\bold F$ is a rectangular matrix,
then it is certain that one of the two is not invertible.
(There are plenty of real cases where neither matrix is invertible.
That's one reason we use iterative solvers.)
Here we are dealing with the case with more model points than data points.

\par
Now we will show that of all possible models $\bold m$ that
predict the correct data, $\bold m_0$ has the least energy.
(I'd like to thank Sergey Fomel for this clear and simple proof
that does {\em  not} use Lagrange multipliers.)
First split (\ref{eqn:underdetest}) into an intermediate
result $\bold d_0$ and final result:
\begin{eqnarray}
        \bold d_0 &=& (\bold F \bold F')^{-1} \bold d
	\\
        \bold m_0 &=& \bold F' \bold d_0
\end{eqnarray}
Consider another model ($\bold x$ not equal to zero)
\begin{equation}
        \bold m \eq \bold m_0 + \bold x
\end{equation}
which fits the theoretical data
$\bold d = \bold F(\bold m_0+\bold x)$.
Since
$\bold d=\bold F\bold m_0$,
we see
that $\bold x$ is a null space vector.
\begin{equation}
        \bold F \bold x \eq \bold 0
\end{equation}
First we see that $\bold m_0$ is orthogonal to $\bold x$ because
\begin{equation}
         \bold m_0' \bold x \eq
         (\bold F' \bold d_0)' \bold x \eq
         \bold d_0' \bold F \bold x \eq
         \bold d_0' \bold 0 \eq 0
	\label{eqn:dzero}
\end{equation}
Therefore,
\begin{equation}
\bold m' \bold m \eq
\bold m_0' \bold m_0 + \bold x'\bold x + 2 \bold x' \bold m_0 \eq
\bold m_0' \bold m_0 + \bold x'\bold x \quad\ge\quad \bold m_0' \bold m_0 
\end{equation}
so adding null space to $\bold m_0$ can only increase its energy.
In summary,
the solution
$\bold m_0 = \bold F' (\bold F \bold F')^{-1} \bold d$
has less energy than any other model that satisfies the data.

\par
Not only does the theoretical solution
$\bold m_0 = \bold F' (\bold F \bold F')^{-1} \bold d$
have minimum energy,
but the result of iterative descent will too,
provided that we begin iterations from $\bold m_0=0$ or any $\bold m_0$
with no null-space component.
In (\ref{eqn:dzero}) we see that the 
orthogonality $\bold m_0'\bold x=0$
does not arise because $\bold d_0$ has any particular value.
It arises because $\bold m_0$ is of the form $\bold F'\bold d_0$.
Gradient methods contribute $\Delta\bold m =\bold F' \bold r$
which is of the required form.

%\section{VIRTUAL-RESIDUAL PRECONDITIONING}
%Sergey Fomel has developed a wholly different
%approach to preconditioning to that described above.
%He calls his method the ``model-extension'' method.
%I call it the ``virtual-residual'' method.
%Both names are suggestive of the method.
%\par
%We begin with the usual definition
%of preconditioned variables $\bold m =\bold S\bold p$ and
%the opposite of our usual sign convention for the residual.
%\begin{eqnarray}
%-\epsilon \ \bold r &=&  \bold F         \bold m - \bold d \\
%-\epsilon \ \bold r &=&  \bold F \bold S \bold p - \bold d
%\label{eq:r}
%\end{eqnarray}
%which we arrange with a column vector of unknowns that includes
%not only $\bold p$ but also $\bold r$.
%\begin{equation}
%\bold 0
%\quad\approx\quad \hat {\bold r}
%\eq 
%\left[ \bold F \bold S \quad \epsilon \ \bold I \right]
%\
%\left[
%\begin{array}{c}
%\bold p \\
%\bold r
%\end{array}
%\right]
%\ - \  \bold d
%\end{equation}
%The interesting thing is that we can use our familiar
%conjugate-gradient programs with this system of equations.
%We only need to be careful to distinguish the residual in the bottom half
%of our solution vector,
%from the virtual residual $\hat {\bold r}$
%(which should vanish identically)
%in the iterative fitting problem.
%The fitting begins with the initializations:
%\begin{equation}
%\hat{\bold p}_0  \eq \left[
%\begin{array}{c}
%\bold p_0 \\
%\bold 0
%\end{array}
%\right]
%\end{equation}
%and
%\begin{equation}
%\hat{\bold r}_0  \eq
%      \bold F\bold S \bold p_0 -\bold d \eq
%      \bold F        \bold m_0 -\bold d
%\end{equation}
%As the iteration begins we have gradients
%of the two parts of the model
%\begin{eqnarray}
%\bold g_m &=& \bold S' \bold F' \hat{\bold r} \\
%\bold g_d &=& \epsilon \        \hat{\bold r}
%\end{eqnarray}
%which imply a perturbation in the theoretically zero residual
%\begin{equation}
%\Delta\hat{\bold r}  \eq
%      \bold F\bold S \bold g_m + \epsilon\; \bold g_d
%\end{equation}
%Then step sizes and steps are determined
%as usual for conjugate-direction fitting.
%\par
%The preconditioning module \texttt{vr\_solver}
%%\vpageref{lst:vrsolver}
%takes three functions as its arguments.
%Functions \texttt{oper} and \texttt{prec} correspond to the linear
%operators $\bold F$ and $\bold S$.
%Function \texttt{solv} implements one step of an optimization descent.
%Its arguments are a logical parameter \texttt{forget},
%which controls a conditional restarting of the optimization,
%the current effective model \texttt{x0},
%the gradient vector \texttt{g}, the data residual vector \texttt{rr}, and
%the conjugate gradient vector \texttt{gg}.
%Subroutine \texttt{solver\_vr} constructs
%the effective model vector \texttt{x},
%which consists of the model-space part \texttt{xm}
%and the data-space part \texttt{xd}.
%Similarly, the effective gradient vector \texttt{g}
%is split into the the model-space part \texttt{gm}
%and the data-space part \texttt{gd}.
%\par
%For brevity I am omitting the code
%for the virtual-residual solver \texttt{vrsolver}
%which is in the library.
%It follows the same pattern as
%\texttt{prec\_solver} \vpageref{lst:precsolver}.
%%\moddex{vrsolver}{Virtual-residual solver}

\section{SCALING THE ADJOINT}

First I remind you of a rarely used little bit of mathematical notation.
Given a vector $\bold m$ with components $(m_1,m_2,m_3)$,
the notation $ {\bf diag\ }\bold m$ means
\begin{equation}
 {\bf diag\ }\bold m \eq
	\left[
 	\begin{array}{ccc}
		m_1 &  0 & 0 
		\\
		0   &m_2 & 0
		\\
		0   &  0 & m_3
	\end{array}
	\right]
\end{equation}
\par
Given the usual linearized fitting goal between
data space and model space, $ \bold d \approx \bold F \bold m$,
the simplest image of the model space results from
application of the adjoint operator $ \hat{\bold m} = \bold F' \bold d$.
Unless $\bold F$ has no physical units, however,
the physical units of $\hat{\bold m}$ do not match those of $\bold m$,
so we need a scaling factor.
The theoretical solution
$\bold m_{\rm theor} = (\bold F'\bold F)^{-1}\bold F'\bold d$
tells us that the scaling units should be those of $(\bold F'\bold F)^{-1}$.
We are going to approximate $(\bold F'\bold F)^{-1}$ by a diagonal matrix
$\bold W^2$ with the correct units so
$\hat{\bold m} = \bold W^2 \bold F'\bold d$.

\par
What we use for $\bold W$ will be a guess, simply a guess.
If it works better than nothing, we'll be happy,
and if it doesn't we'll forget about it.
Experience shows it is a good idea to try.
Common sense tells us to insist that all elements of $\bold W^2$ are positive.
$\bold W^2$ is a square matrix of size of model space.
From any vector $\tilde {\bold m}$ in model space with all positive components,
we could guess that $\bold W^2$ be
${\bf diag\ } \tilde {\bold m}$ to any power.
To get the right physical dimensions we choose
$\tilde {\bold m}=\bold 1$, a vector of all ones and choose
\begin{equation}
\bold W^2 \eq
	{
		1
			\over
		{\bf diag\ } {\bold F'\bold F\bold 1}
	}
\label{eqn:adjointwt}
\end{equation}
A problem with the choice
(\ref{eqn:adjointwt}) is that some components might be zero or negative.
Well, we can take the square root of the squares of components
and/or smooth the result.

\par
To go beyond the scaled adjoint we can use $\bold W$ as a \bx{precondition}er.
To use $\bold W$ as a preconditioner
we define implicitly a new set of variables $\bold p$
by the substitution $\bold m=\bold W\bold p$.
Then $\bold d \approx \bold F\bold m=\bold F\bold W\bold p$.
To find $\bold p$ instead of $\bold m$,
we iterate
with the operator $\bold  F\bold W$ instead of with $\bold F$.
As usual, the first step of the iteration is to use the adjoint
of $\bold d\approx \bold F\bold W\bold p$ to form the image
$\hat{\bold p}=(\bold F\bold W)'\bold d$.
At the end of the iterations,
we convert from  $\bold p$ back to  $\bold m$
with $\bold m=\bold W\bold p$.
The result after the first iteration
$\hat{\bold m}=\bold W\hat{\bold p}=\bold W(\bold F\bold W)'\bold d=\bold W^2\bold F'\bold d$
turns out to be the same as scaling.
\par
By (\ref{eqn:adjointwt}), $\bold W$ has physical units inverse to $\bold F$.
Thus the transformation $\bold F\bold W$ has no units
so the $\bold p$ variables have physical units of data space.
Experimentalists might enjoy seeing the 
solution $\bold p$
with its data units more than viewing the solution $\bold m$
with its more theoretical model units.

\par
The theoretical solution for underdetermined systems
         $\bold m =\bold F' (\bold F \bold F')^{-1} \bold d$
suggests
an alternate approach using instead
         $\hat{\bold m} =\bold F' \bold W_d^2 \bold d$.
This diagonal weighting matrix $\bold W_d^2$ must be drawn
from vectors in data space.
Again I chose a vector of all 1's getting the weight
\begin{equation}
\bold W_d^2 \eq
	{
		1
			\over
		{\bf diag\ } {\bold F\bold F'\bold 1}
	}
\label{eqn:datawtadjoint}
\end{equation}

\par
My choice of a vector of 1's is quite arbitrary.
I might as well have chosen a vector of random numbers.
Bill Symes, who suggested this approach to me,
suggests using an observed data vector $\bold d$ for the data space weight,
and $\bold F' \bold d$ for the model space weight.
This requires an additional step, dividing out the units of the data $\bold d$.

\par
Experience tells me that a broader methodology than all above is needed.
Appropriate scaling is required in both data space and model space.
We need two other weights
$\bold W_m$ and
$\bold W_d$ where
$\hat{\bold m} = \bold W_m \bold F'\bold W_d \bold d$.

%\par
%I have a useful practical example (stacking in $v(z)$ media)
%in another of my electronic books (BEI),
%where I found both
%$\bold W_m$ and
%$\bold W_d$ by iterative guessing.
%First assume $\bold W_d =\bold I$ and estimate $\bold W_m$ as above.
%Then assume you have the correct $\bold W_m$ and estimate $\bold W_d$ as above.
%Iterate.
%Perhaps some theorist can find a noniterative solution.

\par
I have a useful practical example (stacking in $v(z)$ media)
in another of my electronic books (BEI),
where I found both
$\bold W_m$ and
$\bold W_d$ by iterative guessing.
First assume $\bold W_d =\bold I$ and estimate $\bold W_m$ as above.
Then assume you have the correct $\bold W_m$ and estimate $\bold W_d$ as above.
Iterate.
(Perhaps some theorist can find a noniterative solution.)
I believe this iterative procedure leads us to the best diagonal
pre- and post- multipliers for any operator $\bold F$.  
By this I mean that the modified operator 
$(\bold W_d \bold F \bold W_m)$
is as close to being unitary as we will be able to obtain
with diagonal transformation.
Unitary means it is energy conserving and that the inverse
is simply the conjugate transpose.
\par
What good is it that
$(\bold W_d \bold F \bold W_m)'
(\bold W_d \bold F \bold W_m) \approx \bold I$?
It gives us the most rapid convergence of least squares problems of the form
\begin{equation}
\label{eqn:genform}
\bold 0 \quad\approx\quad \bold W_d ( \bold F\bold m - \bold d)
\eq               \bold W_d ( \bold F\bold W_m \bold p - \bold d)
\end{equation}
Thus it defines for us the best
diagonal transform to a
preconditioning variable
$\bold p=\bold W_m^{-1}\bold m$ to use during iteration,
and suggests to us what residual weighting function we need to use if
rapid convergence is a high priority.
Suppose we are not satisfied with $\bold W_d$ being the weighting function
for residuals.  Equation~(\ref{eqn:genform}) could still help us speed iteration.
Instead of beginning iteration with $\bold p=\bold 0$,
we could begin from the solution $\bold p$
to the regression~(\ref{eqn:genform}).

\par
The PhD thesis of James Rickett experiments extensively
with data space and model space weighting functions
in the context of seismic velocity estimation.


\section{A FORMAL DEFINITION FOR ADJOINTS}

In mathematics, adjoints are defined a little differently than
we have defined them here
(as matrix transposes).\footnote{
	I would like to thank Albert Tarantola for suggesting this topic.
	}
The mathematician begins by telling us that we cannot simply
form any dot product we want.
We are not allowed to take the dot product of any two vectors
in model space
$\bold m_1 \cdot \bold m_2$ or data space
$\bold d_1 \cdot \bold d_2$.
Instead, we must first transform them to a preferred coordinate system.
Say
$\tilde{\bold m}_1 = \bold M \bold m_1$ and
$\tilde{\bold d}_1 = \bold D \bold d_1$, etc for other vectors.
We complain we do not know $\bold M$ and $\bold D$.
They reply that we do not really need to know them
but we do need to have the inverses (aack!) of
$\bold M'\bold M$ and
$\bold D'\bold D$.
A pre-existing common notation is
$\bold\sigma_m^{-2} = \bold M'\bold M$ and
$\bold\sigma_d^{-2} = \bold D'\bold D$.
Now the mathematician buries the mysterious new positive-definite
matrix inverses in the definition of dot product
$<\bold m_1,\bold m_2> = \bold m'_1 \bold M'\bold M \bold m_2 = \bold m'_1 \bold \sigma_m^{-2} \bold m_2$
and likewise with
$<\bold d_1,\bold d_2>$.
This suggests a total reorganization of our programs.
Instead of computing
$(\bold m'_1 \bold M') (\bold M \bold m_2)$
we could compute $\bold m'_1 (\bold\sigma_m^{-2} \bold m_2)$.
Indeed, this is the ``conventional'' approach.
This definition of dot product would be buried in the solver code.
The other thing that would change would be the search direction
$\Delta \bold m$.
Instead of being the gradient as we have defined it
$\Delta \bold m=\bold L'\bold r$,
it would be
$\Delta \bold m=\bold\sigma_m^{-2} \bold L'\bold\sigma_d^{-2}\bold r$.
A mathematician would
{\em define} the adjoint of $\bold L$ to be
$\bold\sigma_m^{-2} \bold L'\bold\sigma_d^{-2}$.
(Here $\bold L'$ remains matrix transpose.)
You might notice this approach nicely incorporates
both residual weighting and preconditioning
while yet evading the issue of where we get the matrices
$\sigma_m^2$ and $\sigma_d^2$ or how we invert them.
Fortunately, upcoming chapter \ref{paper:mda}
suggests how,
in image estimation problems,
to obtain sensible estimates of the elusive operators $\bold M$ and $\bold D$.
Paranthetically, modeling calculations in physics and engineering
often use similar mathematics
in which the role of $\bold M'\bold M$ is not so mysterious.
Kinetic energy is mass times velocity squared.
Mass can play the role of $\bold M'\bold M$.

\par
So, should we continue to use
$(\bold m'_1 \bold M') (\bold M \bold m_2)$
or should we take the conventional route and go with
$\bold m'_1 (\bold\sigma_m^{-2} \bold m_2)$?
One day while benchmarking a wide variety of computers I was shocked
to see some widely differing numerical results.  Now I know why.
Consider adding $10^7$ identical positive floating point numbers, say 1.0's,
in an arithmetic with precision of $10^{-6}$.
After you have added in the first $10^6$ numbers,
the rest will all truncate in the roundoff
and your sum will be wrong by a factor of ten.
If the numbers were added in pairs,
and then the pairs added, etc, there would be no difficulty.
Precision is scary stuff!

\par
It is my understanding and belief that there is nothing wrong
with the approach of this book, in fact,
it seems to have some definite advantages.
While the conventional approach requires one
to compute the adjoint correctly, we do not.
The method of this book
(which I believe is properly called conjugate directions)
has a robustness that, I'm told,
has been superior in some important geophysical applications.
The conventional approach seems to get in trouble when
transpose operators are computed with insufficient precision.



%On the other hand, I can envision applications where
%the conventional approach would be preferable.
%The matrix $\bold M$ is the size of model space times the size of data space
%while the matrix $\bold\sigma_m^{-2}$ is the size of model space squared.
%Clearly, there is the potential for some applications to speed up
%by switching from our approach to the conventional approach.



%\section{ACKNOWLEDGEMENTS}
%Nearly everything I know about null spaces
%I learned from Dave Nichols, Bill Symes, Bill Harlan, and Sergey Fomel.

\unboldmath
%\activeplot{name}{width=6in,height=}{}{caption}
%\activesideplot{name}{height=1.5in,width=}{}{caption}
%
%\begin{equation}
%\label{eqn:}
%\end{equation}
%
%\ref{fig:}
%(\ref{eqn:})




%\end{notforlecture}

\clearpage
