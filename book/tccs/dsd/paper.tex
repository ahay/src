\published{Geophysics, 81, no. 2, V17-V30, (2016)}
\title{Double sparsity dictionary for seismic noise attenuation}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\author{Yangkang Chen\footnotemark[1], Jianwei Ma\footnotemark[2] and Sergey Fomel\footnotemark[1]}

\ms{GEO-2014-0525}

\address{
\footnotemark[1]Bureau of Economic Geology \\
John A. and Katherine G. Jackson School of Geosciences \\
The University of Texas at Austin \\
University Station, Box X \\
Austin, TX 78713-8924 \\
ykchen@utexas.edu \& sergey.fomel@beg.utexas.edu \\
\footnotemark[2] Department of Mathematics \\
Harbin Institute of Technology \\
Harbin, China \\
jma@hit.edu.cn
}

\lefthead{Chen \& Ma \& Fomel}
\righthead{Double sparsity dictionary}
\footer{TCCS-10}

\maketitle

\begin{abstract}
A key step in sparsifying signals is the choice of a sparsity-promoting dictionary. There are two basic approaches to design such a dictionary: the analytic approach and the learning-based approach. While the analytic approach enjoys the advantage of high efficiency, it lacks adaptivity to various data patterns. On the other hand, the learning-based approach can adaptively sparsify different datasets but has a heavier computational complexity and involves no prior-constraint pattern information for particular data. We propose a double sparsity dictionary (DSD) for seismic data in order to combine the benefits of both approaches. We provide two models to learn the DSD: the synthesis model and the analysis model. The synthesis model learns DSD in the data domain, and the analysis model learns DSD in the model domain. We give an example of the analysis model and propose to use the seislet transform and data-driven tight frame (DDTF) as the base transform and adaptive dictionary respectively in the DSD framework.  DDTF obtains an extra structure regularization by learning dictionaries, while the seislet transform obtains a compensation for the transformation error caused by slope dependency. The proposed DSD aims to provide a sparser representation than the individual transform and dictionary and therefore can help achieve better performance in denoising applications. Although for the purpose of compression, the proposed DSD is less sparse than the seislet transform, it outperforms both seislet and DDTF in distinguishing signal and noise. Two simulated synthetic examples and three field data examples confirm a better denoising performance of the proposed approach.
\end{abstract}


\section{INTRODUCTION}
Sparse approximations aim at representing the most information of the given data by a linear combination of pre-specified atom signals with sparse linear coefficients. Sparse approximation theory has been a rapidly evolving field, since many state-of-the-art signal and image processing tasks have been successfully handled with the concept of sparse representation, including image inpainting and restoration \cite[]{elad2005,mairal2008,mairal2009,youli2011,jianfeng2013}, image denoising \cite[]{protter2009,jianfeng2013}, data compression \cite[]{bryt2008}, and blind source separation \cite[]{zib2001}. The majority of known methods for sparse approximation of signals can be divided into two general categories: an analytic approach and a learning-based approach. The analytic approach refers to a fixed basis while the learning-based approach adaptively finds the required sparse basis by training. \emph{Transform} is usually applied to refer to analytic approach while \emph{dictionary} refers to learning-based approach. A number of sparsity promoting transforms with fixed basis functions have been proposed in the literature for handling different signal processing tasks, including wavelets \cite[]{sweldens1995,mallat2009}, curvelets \cite[]{candes20061,jianwei2010}, contourlets \cite[]{do2005}, shearlets \cite[]{labate2005}, and bandelets \cite[]{lepennec2005}, as well as the classic Radon transforms \cite[]{amr2014,amr20142}. The learning-based dictionaries usually utilize machine learning techniques to infer the dictionary. The advantage of this type of approach is finer-tuned dictionaries compared with the analytic approach. Tuned dictionaries can result in better performance in different applications. The downside of the learning-based approach is a higher computational cost because of a large number of iterations and redundant overlap computations for small patches. Different algorithms for learning dictionaries may result in different efficiencies and performances. Commonly used dictionary-training algorithms include principle component analysis (PCA) \cite[]{jolliffe2002,vidal2005}, the method of optimal directions (MOD) \cite[]{engan1999}, and variants of singular-value decomposition (SVD) such as K-SVD \cite[]{aharon2006}. 

Over the past several decades, different types of fixed-basis sparsity-promoting transforms have been explored for seismic data processing applications, and promising results have been reported. \cite{luoyi1992} applied a wavepacket transform to seismic data compression. \cite{zhangr2003} developed a type of wavelet frame that takes into account the characteristics of seismic data both in time and space for denoising applications. \cite{ioup1998} applied a wavelet transform to both random noise removal and data compression using soft thresholding in the wavelet domain. \cite{du2000} applied a multi-resolution property of wavelet transform to attenuate tube waves. \cite{jafarpour2009} used a discrete cosine transform to obtain sparse representations of fields with distinct geologic features and to improve the solutions of traditional geophysical estimation problems.%\cite{herrmann2010} utilized the sparse representation of seismic in curvelet domain to reconstruct missing traces from the view of compressive sensing \cite[]{donoho2006}. 
A number of researchers have reported successful applications of the curvelet transform in coherent and random noise attenuation thanks to the multi-scale directional property of the curvelet domain \cite[]{hennenfent2006,deli2008,neelamani2008,neelamani2010}. %\cite{deli2008} inserted the sparsity-promoting property of curvelet transform into Bayesian inversion framework to separate the primary and multiple reflections. 
Following the compressive sensing theory \cite[]{donoho2006}, curvelet transform has also been used to restore missing seismic data \cite[]{hennenfent2010,mostafa2010}.  The fixed-basis sparse transforms enjoy efficiency benefits. However, they are not data-adaptive.

\cite{fomel2010seislet} introduced a sparsity promoting transform which can be data adaptive, called the \emph{seislet} transform. Following the lifting scheme used in the construction of second-generation wavelets, the seislet transform utilizes the spatial predictability property of seismic data to construct the predictive operator. \cite{fomel2010seislet} used plane-wave destruction \cite[]{fomel2002pwd} to aid the prediction process. Instead of using the plane-wave destruction algorithm, \cite{liuyang2010} used differential offset-continuation to construct seislet transform for prestack reflection data. The offset continuation seislet transform can obtain better sparsity for conflicting-dip events in prestack data by using offset continuation instead of local slopes to connect different common offset gathers. %however, at the expense of much higher computational cost. 

The learning-based dictionaries have not been widely used in seismic data processing until recent years. \cite{kaplan2009} used a data-driven sparse-coding algorithm to adaptively learn basis functions in order to sparsely represent seismic data in the transform domain and performed improved denoising performance for both synthetic and field data examples. Based on a variational sparse-representation model, \cite{jianwei20142} proposed a denoising approach by adaptively learning dictionaries from noisy seismic data.
\cite{jianwei2014} applied a learning-based sparsity-promoting dictionary, called data-driven tight frame (DDTF) \cite[]{jianfeng2013}, as a sparse transform in the framework of the split inexact Uzawa algorithm to restore missing seismic data. The split inexact Uzawa algorithm was proposed by \cite{xiaoqun2011} and can be viewed as a generalization of the alternating direction of multiplier method (ADMM). While learning-based dictionaries can be more adaptive than fixed-basis transforms, no prior-constraint structural information is involved in the construction of these dictionaries. As seismic data have distinct structural patterns, the general learning-based dictionaries can be improved by incorporating structure information for achieving additional sparsity.

In this paper, we propose a double sparsity dictionary (DSD) for sparsifying seismic data. The basic concept of DSD is borrowed from the image processing literature \cite[]{ron2010,ophir2011}. \cite{ron2010} initialized the basic principle of a sparsity model of the dictionary atoms over a base dictionary. \cite{ophir2011} extended the method of \cite{ron2010} largely by grouping the analysis-domain data into bands. By bridging the gap between fixed-basis transforms and learning-based dictionaries, we propose a cascaded DSD framework, which aims at combining the benefits of both approaches. The proposed DSD framework offers an extra level of sparsity for representing seismic data. On the one hand, DSD compensates the weakness of fixed-basis transforms in sparsifying seismic data patterns and thus can be more robust. At the same time, DSD can relieve the dependence of learning-based dictionaries on large filtering-window overlap and large number of iterations and thus can be more efficient and effective. We define two models to construct DSD: a synthesis model and an analysis model; according to the learning domain of each model. %The synthesis model is more general, while the analysis model can be more conveniently implemented. 
We provide an example of the analysis model, in which we cascade the seislet transform and DDTF to form a DSD framework. In this framework, DDTF  compensates for the dip dependence of the seislet transform, while the seislet transform adds structural regularization to DDTF. Thus, the proposed DSD can provide sparser representation of seismic data in order to implement thresholding-based denoising algorithms. We use two synthetic tests and three field data examples to test the performance of the proposed DSD framework in removing random noise. Our experiments show noticeably better performance using DSD-based thresholding than using either seislet or DDTF based thresholdings. For the presented examples, DSD also performs significantly better than the classic $f$-$x$ deconvolution, according to both SNR measurements and visual observations.
% The construction of double-sparsity dictionary can be realized by cascading two process. Instead of learning the basis of data, we turns to adaptively learn the basis of coefficients in the fixed-basis transform domain. This double-sparsity framework offers an extra level of sparsity to sparsely represent seismic data. On one hand, the double sparsity dictionary compensate the weakness of fixed-basis transform in sparsifying complicated-structure seismic profiles and thus can be more robust. On another hand, the double sparsity dictionary relieve the dependence of learning-based dictionary on large filtering-window overlap and large number of iterations and thus can be more efficient and effective.



%which has find successful applications both in seismic denoising and seismic data restoration. 


%1. sparse representation 
%2. constant basis dictionary
%3. adaptive basis dictionary
%4. two paragraph for this paper


\section{Theory}
\subsection{Double sparsity dictionary}
Sparse representation for a given signal $\mathbf{d}$ can be formulated as the following optimization problem \cite[]{ron2010,ophir2011}:
\begin{equation}
\label{eq:eq1}
\hat{\mathbf{m}} = \arg \min_{\mathbf{m}} \parallel\mathbf{m} \parallel_0, s.t. \parallel \mathbf{d}-\mathbf{A}\mathbf{m}\parallel_2 \le \epsilon.
\end{equation}
Here, $\mathbf{m}$ is the sparse representation of the observed signal $\mathbf{d}$, $\mathbf{A}$ is the sparsity-promoting transform (or dictionary), and $\hat{\mathbf{m}}$ is the estimated sparse representation.
$\parallel \cdot \parallel_0$ and $\parallel \cdot \parallel_2$ denote the $L_0$ norm and $L_2$ norm of a vector, respectively, and $\epsilon$ is the error tolerance.
% $L0$ norm counts the number of nonzero components of the vector
The double sparsity model states that the sparsity promoting transform $\mathbf{A}$ can be implemented as a cascaded combination of two dictionaries:
\begin{equation}
\label{eq:eq2}
\mathbf{A} = \mathbf{BW},
\end{equation}
where $\mathbf{B}$ is a base analytic transform and $\mathbf{W}$ is an adaptive learning-based dictionary.  The base dictionary can be a fixed-basis sparsity-promoting dictionary, while the adaptive dictionary can be adaptively learned over the compact representation provided by $\mathbf{B}$. 

Compared with a fixed-basis dictionary, the double sparsity model as defined in equation \ref{eq:eq2} can provide more adaptability by modifying $\mathbf{W}$. It acts as an extension to the initial base dictionary $\mathbf{B}$. Compared with learning-based dictionary, the double sparsity model can be more efficient and stable because the data to be learned have a relatively more compact representation and thus the initial base transform provides a structure regularizer to the learning process. The double-sparsity transform domain coefficients $\mathbf{m}$ obtain two-level sparsity: base transform provides the first-level sparsity and the learning dictionary provides the second-level sparsity.  In this respect, the philosophy of our work is also similar to the bandlets \cite[]{lepennec2005}: rather than seeking the direct transform to get the ultimate sparsification of the input signals, one can first use an existing transform that does reasonably well in that respect, and then add another layer of processing that squeezes more over the already simplified signals \cite[]{ophir2011}. In the next two sections, we introduce two models to learn the double sparsity dictionary (DSD) for seismic data: synthesis-based DSD and analysis-based DSD.

%\subsection{Learning DSD in image domain - synthesised-based DSD}
\subsection{Learning DSD in data and model domains - synthesis and analysis models}
Provided that the learning-based dictionary is a tight frame such that $(\mathbf{W}^T)^T\mathbf{W}^T=\mathbf{W}\mathbf{W}^T=\mathbf{I}$,
DSD can be learned by considering the following problem:
\begin{equation}
\label{eq:eq3}
\begin{split}
&\min_{\mathbf{W,m}}  \frac{1}{2}\parallel \mathbf{d} - \mathbf{BWm} \parallel_2^2, \\
&s.t. \parallel \mathbf{m} \parallel_0 \le t \quad  and \quad \mathbf{W}\mathbf{W}^T = \mathbf{I},
\end{split}
\end{equation}
where $t$ denotes the sparsity constrained coefficient.

Equation \ref{eq:eq3} can be solved alternatively by minimizing
\begin{equation}
\label{eq:synthesis}
\begin{split}
\min_{\mathbf{W},\mathbf{m}} \frac{1}{2}&\parallel \mathbf{d} - \mathbf{BWm}\parallel_2^2 + \lambda\parallel \mathbf{m} \parallel_0, \\
&s.t. \mathbf{W}\mathbf{W}^T = \mathbf{I},
\end{split}
\end{equation}
where $\lambda$ denotes the damping factor, which is connected with the sparsity constrained coefficient $t$. Equation \ref{eq:synthesis} is called \emph{synthesis} model for DSD.

Assuming the base dictionary $\mathbf{B}$ is invertible, the synthesis-based DSD model as shown in equation \ref{eq:eq3} is equivalent to:
\begin{equation}
\label{eq:analysis}
\begin{split}
\min_{\mathbf{W,m}}  \frac{1}{2}&\parallel \mathbf{B}^{-1}\mathbf{d} - \mathbf{Wm} \parallel_2^2 + \lambda\parallel \mathbf{m} \parallel_0 \\
&s.t. \mathbf{W}\mathbf{W}^T = \mathbf{I},
\end{split}
\end{equation}
where $\mathbf{B}^{-1}$ denotes the forward base transform. Equation \ref{eq:analysis} suggests that DSD can also be learned in the model domain of the multi-scale decomposition operator $\mathbf{B}$ instead of in the data domain. Equation \ref{eq:analysis} is called \emph{analysis} model for DSD.

The synthesis model (equation \ref{eq:synthesis}) offers more flexibility for learning DSD because there are many sparsity-promoting transforms that are not exactly invertible. The analysis model, on the other hand, offers more convenience for constructing DSD when an invertible sparsity-promoting 
base transform is available. 
%In order to solve equation \ref{eq:synthesis}, we use the data-driven approach that is used in \cite{jianfeng2013}. Supposing $\{\mathbf{f}_i\}_i^M$ are filters associated with the construction of $\mathbf{W}$, then instead of solving equation \ref{eq:synthesis} for $\mathbf{m}$ and $\mathbf{W}$ directly, we turn to solve the following equation for $\mathbf{m}$ and $\{\mathbf{f}_i\}_{i=1}^{M}$:

%\begin{equation}
%\label{eq:eq44}
%\begin{split}
%\min_{\{\mathbf{f}_i\}_{i=1}^{M},\mathbf{m}} \frac{1}{2}&\parallel \mathbf{d} - \mathbf{BWm}\parallel_2^2 + \lambda\parallel \mathbf{m} \parallel_0, \\
%&s.t. \mathbf{W}^T\mathbf{W} = \mathbf{I}.
%\end{split}
%\end{equation}

%The synthesis operator $\mathbf{W}^T\in \mathbf{R}^{N\times NM}$ is defined as
%\begin{equation}
%\label{eq:eq5}
%\mathbf{W}^T = [\mathcal{C}_{\mathbf{f}_1}, \mathcal{C}_{\mathbf{f}_2},\cdots,\mathcal{C}_{\mathbf{f}_M}],
%\end{equation}
%where $\mathcal{C}_{\mathbf{f}}: \mathbf{R}^N \rightarrow \mathbf{R}^N$ is the block-wise Toeplitz matrix that represents the convolution operator with a finitely supported 2D filter $\mathbf{f}$ under Newmann boundary condition \cite[]{jianfeng2013}. The analysis operator $\mathbf{W}\in \mathbf{R}^{NM\times N}$ is

%\begin{equation}
%\label{eq:eq6}
%\mathbf{W} = [\mathcal{C}^T_{\mathbf{f}_1(-)}, \mathcal{C}^T_{\mathbf{f}_2(-)},\cdots,\mathcal{C}^T_{\mathbf{f}_M(-)}]^T,
%\end{equation}

%The minimization can be solved by updating coefficients vector $\mathbf{m}$ and filters $\{\mathbf{f}_i\}_{i=1}^{M}$ alternately. To be clearer, 
%we provide the algorithm for solving problem \ref{eq:synthesis} as follows\\
%Input: Base dictionary $\mathbf{B}$, initial filters $\{\mathbf{f}^{(0)}_i\}_{i=1}^M$, \\
%\textbf{for} $k=0,1,\cdots,K-1$
%\begin{enumerate}
%\item Fix the frame filters $\{\mathbf{f}^{(0)}_i\}_{i=1}^M$, estimate the double-sparsity coefficients vector $\mathbf{m}^{(k)}$ by
%\begin{equation}
%\label{eq:eq7}
%\mathbf{m}^{(k)}=\arg\min_{\mathbf{m}} \parallel \mathbf{d}-\mathbf{B}\mathbf{W}^{(k)}\mathbf{m} %\parallel_2^2 + \lambda\parallel \mathbf{m} \parallel_0,
%\end{equation}
%where $\mathbf{W}^{(k)}$ is the analysis operator related with filters $\{\mathbf{f}^{(k)}_i\}_{i=1}^M$ according to equation \ref{eq:eq6}.
%\item Given the double-sparsity coefficients vector $\mathbf{m}^{(k)}$, update the frame filters $%\{\mathbf{f}^{(k+1)}_i\}_{i=1}^M$ by:
%\begin{equation}
%\label{eq:eq8}
%\begin{split}
%\{\mathbf{f}^{(k)}_i\}_{i=1}^M &= \arg\min_{\{\mathbf{f}_i\}_{i=1}^M} \parallel \mathbf{d} - \mathbf{B}\mathbf{W}(\mathbf{f}_1,\mathbf{f}_2,\cdots,\mathbf{f}_M)\mathbf{m}^{(k)}\parallel_2^2, \\
%&s.t. \mathbf{W}^{T}\mathbf{W} = \mathbf{I}_N
%\end{split}
%\end{equation}
%where $\mathbf{W}^T$ is the synthesis operator related with $\{\mathbf{f}_i\}_{i=1}^M$ according to equation \ref{eq:eq5}.
%\end{enumerate}
%\textbf{end for}
%After $K$ iterations, the double-sparsity dictionary can be obtained and the observed data can be sparsely represented by DSD.

%\subsection{Learning DSD in model domain - analysis model}
\subsection{Solving the analysis model by data-driven approach}
In order to perform optimization in equations \ref{eq:synthesis} and \ref{eq:analysis}, we can employ the data-driven approach that was suggested previously by \cite{jianfeng2013}. As an example, we only introduce the solver for equation \ref{eq:analysis}. %Supposing $\{\mathbf{f}_i\}_i^M$ are filters associated with the construction of $\mathbf{W}$, then instead of solving equation \ref{eq:analysis} for $\mathbf{m}$ and $\mathbf{W}$ directly, we turn to solve the following equation for $\mathbf{m}$ and $\{\mathbf{f}_i\}_{i=1}^{M}$:

The minimization can be solved by updating coefficients of vector $\mathbf{m}$ and the learning-based dictionary $\mathbf{W}$ alternately. We adopt the following algorithm for solving problem \ref{eq:analysis}:\\
%This model is called analysis model in this paper. For this model, we propose the following algorithm:\\
Input: Base dictionary $\mathbf{B}$, initial learning-based dictionary $\mathbf{W}^{(0)}$.
\begin{enumerate}
\item Transform data from data domain to model domain according to
\begin{equation}
\label{eq:ana}
\mathbf{b} = \mathbf{B}^{-1} \mathbf{d}\;.
\end{equation}

\item \textbf{for} $k=0,1,\cdots,K-1$:
\begin{enumerate}
\item
Fix the learning-based dictionary $\mathbf{W}^{(k)}$, estimate the double-sparsity coefficients vector $\mathbf{m}^{(k)}$ by
\begin{equation}
\label{eq:eq7}
\mathbf{m}^{(k)}=\arg\min_{\mathbf{m}} \frac{1}{2}\parallel \left(\mathbf{W}^{(k)}\right)^T \mathbf{b}-\mathbf{m} \parallel_2^2 + \lambda\parallel \mathbf{m} \parallel_0.
\end{equation}

\item Given the double-sparsity coefficients vector $\mathbf{m}^{(k)}$, update the learning-based dictionary $\mathbf{W}^{(k+1)}$:

\begin{equation}
\label{eq:eq8}
\begin{split}
\mathbf{W}^{(k+1)} &= \arg\min_{\mathbf{W}} \frac{1}{2}\parallel \mathbf{W}^T\mathbf{b} - \mathbf{m}^{(k)}\parallel_2^2, \\
&s.t. \mathbf{W}\mathbf{W}^{T} = \mathbf{I}.
\end{split}
\end{equation}
\end{enumerate}
\textbf{end for}
\end{enumerate}
After $K$ iterations, the DSD coefficients are obtained and the observed data become sparsely represented by DSD.
It is known that minimization \ref{eq:eq7} has a unique solution $\hat{\mathbf{m}}$ provided by applying a hard thresholding operator on the coefficient vector $\left(\mathbf{W}^{(k)}\right)^T\mathbf{b}$. The minimization \ref{eq:eq8} can be implemented by using a SVD-based optimization approach \cite[]{jianfeng2013}. 

Assuming the size of the coefficients domain is $N_1\times N_2$, and $N=N_1N_2$, let a filter mapping $\mathbf{F}_\mathbf{a}:\mathcal{R}^N\rightarrow \mathcal{R}^N $ be the block-wise Toeplitz matrix representing the convolution operator with a finitely supported 2D filter $\mathbf{a}$ under the Newmann boundary condition. The learning based dictionary $\mathbf{W}\in \mathcal{R} ^{N\times Np}$ can be defined as
\begin{equation}
\label{eq:Wform}
\mathbf{W} = \left[\mathbf{F}_{\mathbf{a}_1},\mathbf{F}_{\mathbf{a}_2},\cdots,\mathbf{F}_{\mathbf{a}_p}  \right].
\end{equation}
Each $\mathbf{a}_i$ is a 2D filter associated with a tight frame and the columns of $\mathbf{W}$ form a tight frame for $\mathcal{R}^N$. $p$ denotes the number of filters. The patch size discussed in the following examples corresponds to the size of each $\mathbf{a}_i$.

\cite{jianwei2014} give an example of using spline wavelets for the initial $\left(\mathbf{W}^{(0)}\right)^T$ and the finally learned dictionary $\left(\mathbf{W}^{(K)}\right)^T$. Following \cite{jianwei2014}, we also choose spline wavelets for the initial $\mathbf{W}$.

\subsection{Seislet transform based DSD}
%It's worth to be mentioned that when the base transform $\mathbf{B}$ is invertible, the synthesis model and analysis model are equivalent. 
%The learning approach used in the above models to learn DSD was previous used to learn the data-driven tight frames (DDTF) that has been used widely in seismic data processing \cite[]{jianwei2014}. 
The analysis model can be used only if the base transform is invertible and can be more convenient to straightforwardly cascade two types of transforms: the base transform and the learning-based dictionary. In this paper, we use the seislet transform \cite[]{fomel2010seislet} as the base transform and DDTF \cite[]{jianfeng2013} as the learning-based dictionary to construct the proposed DSD framework, following 
the aforementioned algorithm. The DDTF refers exactly to the process of solving equations \ref{eq:eq7} and \ref{eq:eq8}. In the seislet based DSD, the seislet dictionary $\mathbf{B}$ is substituted by the combined dictionary $\mathbf{BW}$. The effective atoms we obtain are thus interpolated version of the atoms in the learning-based dictionary DDTF, interpolated by the seislet transform process. The seislet based DSD thus enjoys the structure-oriented multi-scale capabilities of the seislet transform while adding information to it specific to the training domain. It should also be mentioned that because of the initial seislet decomposition, each sub-band in the transform domain can be treated separately and thus can has its own sub-dictionary \cite[]{ophir2011}.

We train the filters patch by patch of the whole dataset. We apply the maximally overlapping patches (i.e., the two neighbor patches only shifted by one column) for the training. This creates a richness in the training data and leads to a shift-invariance for the dictionary. In the DDTF, for each patch, the training filer is orthogonal, but the full transform matrix $\mathbf{W}$ (each column is a training filter on patches) is redundant. Because of the base seislet transform, the DDTF trains the dictionary from a multi-scale seislet domain instead of the single scale domain of the original data. Although we follow the same strategy as \cite{ophir2011}, we utilize different tools for the two main components in the multi-scale learning framework. The DDTF used in the proposed method is much faster than the K-SVD method used by \cite{ophir2011}, and the seislet transform used in the proposed method is more appropriate for seismic data than the wavelets used by \cite{ophir2011}. In K-SVD, there exists a large number of patches for training and the components of the dictionary are updating individually (indicating many SVD decompositions). However, the DDTF method updates all the components (columns of $\mathbf{W}$) by one SVD decomposition \cite[]{siwei2015}, thus is more efficient. Besides, because of the learning efficiency in DDTF, the DDTF based approaches can be applicable to high-dimensional denoising and interpolation problems. The seislet transform, however, is superior than traditional wavelet transform for compressing seismic data because it utilizes local slope to construct the transform. The superior performance of seislet transform over state-of-the-art transforms in denoising, interpolation, and deblending of seismic data have been demonstrated in a number of references \cite[]{fomel2010seislet,liuyang2010,yangkang20142,shuwei20153}. Thus, the combination between the seislet transform and DDTF through the DSD framework seems very appealing. 



The cascaded DSD framework, combined with thresholding-based denoising, can be briefly summarized as follows:
\begin{enumerate}
\item Apply the seislet transform to noisy seismic data.
\item Apply the DDTF to each band in the seislet domain to form DSD domain.
\item Perform thresholding in the DSD domain.
\item Apply the adjoint DDTF to the thresholded data in DSD domain.
\item Apply the inverse seislet transform to recover the denoised data.
\end{enumerate}

There is an inner iterative process in the above steps when applying the DDTF. It refers to applying the DDTF by the data-driven learning process following equations \ref{eq:ana} to \ref{eq:eq8}. The iterative process acts as a training step for the DDTF, and the trained dictionary $\mathbf{W}$ are finally used to transform the seislet subband data into the double-sparsity domain and used for the adjoint DDTF step in step 4. Figure \ref{fig:demo2} shows a demonstration of denoising processes of the proposed DSD. It also shows the seislet transform domain and DSD transform domain before and after thresholding. 


\inputdir{./}
\plot{demo2}{width=\textwidth}{Demonstration of the DSD-based denoising processes.}


\section{Examples}
In this section, we use two synthetic examples and two field data examples to test the denoising performance of 
the proposed DSD based thresholding. In order to numerically test the performance, and to quantify the 
comparison for all the examples, we simulate noisy data by adding Gaussian white noise.


We apply the following simple model for synthesizing noisy data with Gaussian white noise:
\begin{equation}
\label{eq:eq9}
\mathbf{u} = \mathbf{d} + \epsilon(\sigma),
\end{equation}
where $\mathbf{u}$ denotes the simulated noisy seismic data, $\mathbf{d}$ denotes the clean data, and $\epsilon(\sigma)$ denotes a Gaussian white noise with zero mean and standard deviation $\sigma$. The clean data is recovered by applying a simple thresholding operator in the DSD domain.

In order to numerically test the denoising performance, we define the criterion for comparison as signal-to-noise ratio (SNR):
\begin{equation}
\label{eq:diff}
SNR=10\log_{10}\frac{\Arrowvert \mathbf{d} \Arrowvert_2^2}{\Arrowvert \mathbf{d}-\hat{\mathbf{d}}\Arrowvert_2^2},
\end{equation}
where %$\mathbf{d}$ is the noise-free signal and 
$\hat{\mathbf{d}}$ is the denoised signal.

\subsection{Synthetic examples}
The first synthetic example contains four linear events with different dips. The steepest event crosses the other three events. In order to test the denoising performance in  the presence of non-linear events, we added some continuous random statics to the data by shifting each trace from a linear-events profile with different shifts. The clean seismic profile is shown in Figure \ref{fig:conflict} and the noisy profile with Gaussian white noise added is shown in Figure \ref{fig:conflictn}.

In order to compare the sparsity in compressing seismic data, we plot the reconstruction error curves with respect to the number of selected largest coefficients in Figure \ref{fig:sparse}. The reconstruction error is defined as $E=\parallel\mathbf{d}-\mathbf{Am}_N\parallel_2^2$, where $\mathbf{m}_N$ denotes the largest $N$ coefficients in the transform domain. The faster the reconstruction errors decay, the sparser the corresponding transform domain is. Because both DDTF and DSD are redundant transforms, in order to compare the sparsity in terms of compression fairly, we select a different number of largest coefficients to reconstruct the seismic data and plot the corresponding calculated reconstruction errors. The comparison shows that the reconstruction error curve of the seislet transform decays fastest, followed by DSD and DDTF. For other examples in the paper, the comparison of sparsity in terms of compression behaved similarly.  Thus, we observe that, in terms of compression performance, the seislet transform behaves best, the proposed DSD behaves the second best, while the traditional DDTF behaves worst. Whether the number of the largest coefficients or the percentage of the largest coefficients should be put on the horizontal label is important. Because the space of transform domain might not have the same dimensions for different transforms, we chose to put the number instead of percentage on the horizontal label. When comparing the sparsity in terms of compression, what we really care about is that the least number of coefficients can represent the most information.

In order to compare the sparsity in terms of denoising performance for three different transforms, we compare the best denoised results using both visual observation and measured SNR. We achieve the best denoising performance by modifying the threshold parameter and by comparing the SNR and general denoising performance, while fixing other parameters that might affect the construction of these transforms. Instead of using a fixed threshold value, we use a more flexible percentile thresholding strategy, which means that we apply the inverse transform by using a specific percentage of the  transform domain coefficients sorted by amplitude. The threshold required by the soft- or hard-thresholding in the transform domain is chosen according to the percentage of coefficients we want to preserve. The optimum percentage of coefficients for different approaches might not be the same, therefore we need to set the optimum percentage for all the approaches by trying different percentage to obtain the highest signal-to-noise ratio in order to be fair for all the methods. This strategy was used previously by \cite{yangkang20142}.

Using different percentage coefficients, we can obtain different denoised results for different thresholding approaches. In this example, we use $7\times7$ patch size for the tight frames and  30 iterations for updating the tight frames and the coefficients. Table \ref{tbl:snrcomp1} shows SNRs using different percentage coefficients for three different thresholding approaches. As evident from Table \ref{tbl:snrcomp1}, the seislet thresholding obtains its optimal performance when 5\% coefficients are used. The DDTF thresholding obtains its optimal performance when 4\% coefficients are used. However, the DSD thresholding only needs 2\% coefficients to obtain its optimal performance. The best denoised results are shown in the first row of Figure \ref{fig:conflictn-re,conflictn-ddtf,conflictn-ddtf-re,conflictn-dif,conflictn-ddtf-dif0,conflictn-ddtf-dif,conflictn-err,conflictn-ddtf-err,conflictn-ddtf-re-err}. Their corresponding noise sections are shown in the second row of Figure \ref{fig:conflictn-re,conflictn-ddtf,conflictn-ddtf-re,conflictn-dif,conflictn-ddtf-dif0,conflictn-ddtf-dif,conflictn-err,conflictn-ddtf-err,conflictn-ddtf-re-err}.

Even for the best results using seislet and DDTF thresholdings, there is still a small amount of residual signal left in the noise sections. The recently proposed local orthogonalization of signal and noise \cite[]{yangkang2015ortho} may help address this problem. The local orthogonalization can be used in combination with any denoising approach to recover lost coherent signals in the removed noise section. The seislet thresholding algorithm sometimes suffers from this over-smoothing problem. The slight artifacts as shown in Figure \ref{fig:conflictn-re} are caused by the over-smoothing issue. Figure \ref{fig:conflictn-ddtf-re} shows much better performance in terms of removing the artifacts caused in the seislet transform. Thus, another conclusion of this example is that the DSD can also help improve the result of the traditional seislet transform. Figures \ref{fig:conflictn-err}, \ref{fig:conflictn-ddtf-err}, and \ref{fig:conflictn-ddtf-re-err} show the error sections (difference between the denoised data and the true clean data) of the first example using different approaches. We observe that the DSD thresholding can obtain the least denoising error, which further confirms that the DSD thresholding can preserve more useful energy. For simplicity, we only use a 1\%  increment when scanning the threshold percentage parameters.


The first example demonstrates that the sparsest transform in terms of compression may not be the most efficient for removing random noise. The DSD can achieve a better performance for distinguishing signal and noise. Choosing controlling parameters in order to estimate the best denoised results requires some trial and error in practice. Therefore, from the second synthetic example to the last field data example, we only show the best results for each approach, and calculate the numerical SNRs as a quantitative reference to evaluate the performance. 

\inputdir{conflict}
\multiplot{2}{conflict,conflictn}{width=0.45\columnwidth}{(a) Clean data. (b) Noisy data.}

\plot{sparse}{width=\textwidth}{Sparsity comparison in terms of compression.}

\multiplot{9}{conflictn-re,conflictn-ddtf,conflictn-ddtf-re,conflictn-dif,conflictn-ddtf-dif0,conflictn-ddtf-dif,conflictn-err,conflictn-ddtf-err,conflictn-ddtf-re-err}{width=0.24\textwidth}{First row: denoised sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) DSD thresholding. Second row: noise sections using different approaches. (d) Seislet thresholding. (e) DDTF thresholding. (f) DSD thresholding. Third row: error sections using different approaches. (g) Seislet thresholding. (h) DDTF thresholding. (i) DSD thresholding. The best results are selected for each method.} 



Figure \ref{fig:seislets,ddtfs,dsds} shows a comparison of basis functions of different transforms by randomly selecting a number of significant coefficients in the transform domain, and then transforming the data back to the time-space domain. As can be seen from the comparison, both seislet and DSD have well constructed basis functions following the slope of seismic events. DDTF, however, does not show any structural pattern from the basis functions. DDTF and DSD have better adaptivity than seislet because the seislet basis shows a stronger dependency on the initial slope estimation. In can be observed that the DSD basis (Figure \ref{fig:dsds}) only shows coherent energy around the linear-events regions, while the seislet basis (Figure \ref{fig:seislets}) shows coherent energy nearly everywhere because most regions have zero local slope and such zero local slope will cause coherent basis because of a stronger dip dependency of seislet over DSD. The apparent randomness of the basis of DSD lays between that of seislet and DDTF, which indicates a better adaptivity of DSD to the input dataset. When observing Figure \ref{fig:seislets,ddtfs,dsds}, we should focus on both the randomness and the coherency of the basis, because randomness and coherency represent the slope dependency and data adaptivity, respectively. The DSD obtains a compromise between the two features.  Figure \ref{fig:seisletsr,ddtfsr,dsdsr} shows a comparison of reconstruction performance by selecting 150 maximum coefficients in different transform domains. It is clear that the same number of coefficients in the seislet transform can recover the most amount of useful reflection energy, which is followed by DSD and DDTF, respectively. Figure \ref{fig:seisletsr,ddtfsr,dsdsr} is also consistent with the result shown in Figure \ref{fig:sparse} in that though the DSD can obtain the best performance in separating useful signals and random noise, it is less effective than the seislet transform for compressing the seismic data. Figure \ref{fig:seisletsrperc,ddtfsrperc,dsdsrperc} shows a comparison of reconstruction performance by selecting 0.1\% of the most significant coefficients in different transform domains. Because both DDTF and DSD are redundant, the actual numbers of coefficients in different transform domains are different. For example, in this case, the number of selected coefficients in the seislet domain is 131, but the numbers of selected coefficients in the DDTF and DSD domains are both 6422. It can be seen that in this case the reconstruction using DSD recovers the most information, which is also consistent with Table \ref{tbl:snrcomp1}.  Figures \ref{fig:seisletsr,ddtfsr,dsdsr} and \ref{fig:seisletsrperc,ddtfsrperc,dsdsrperc} also indicate a phenomenon that the largest coefficients in the seislet and DSD domains mainly reconstruct the low-frequency part of the input seismic data, which can be explained by the fact that the low-frequency components are easier for seislet-based transforms to compress.

\inputdir{demo}
\multiplot{3}{seislets,ddtfs,dsds}{width=0.36\textwidth}{Comparison of basis functions by randomly selecting the same number of coefficients in the transform domain. (a) Basis functions of the seislet transform. (b) Basis functions of the DDTF. (c) Basis functions of the DSD.}

\inputdir{demo2}
\multiplot{3}{seisletsr,ddtfsr,dsdsr}{width=0.36\textwidth}{Comparison of reconstruction performance by selecting 150 maximum coefficients in the transform domain. (a) Reconstruction using the seislet transform. (b) Reconstruction using the DDTF. (c) Reconstruction using the DSD.}

\inputdir{demo3.1perc}
\multiplot{3}{seisletsrperc,ddtfsrperc,dsdsrperc}{width=0.36\textwidth}{Comparison of reconstruction performance by selecting 0.1\% largest coefficients in the transform domain. Note that the number of selected coefficients in different transform domains are different because of the redundant nature of both DDTF and DSD. There are 131 coefficients selected in the seislet domain. There are 6422 coefficients selected in both DDTF and DSD domains.  (a) Reconstruction using the seislet transform. (b) Reconstruction using the DDTF. (c) Reconstruction using the DSD.}

In the second example, we use a Sigmoid synthetic model \cite[]{claerbout2010bei}. The clean and noisy data are shown in Figures \ref{fig:sig} and \ref{fig:sign}, respectively. After using soft-thresholding in the seislet domain, DDTF domain, and DSD domain, and selecting the best parameters for each case, we obtain three denoised results, shown in Figure \ref{fig:sign-re,sign-ddtf,sign-ddtf-re}. In this example, we also use $7\times7$ patch size for the tight frames and use 30 iterations for updating the tight frames and the DSD coefficients. The three corresponding noise sections are shown in Figure \ref{fig:sign-dif,sign-ddtf-dif0,sign-ddtf-dif}. The comparison of SNRs for this example is shown in the first row in Table \ref{tbl:snrcomp2}. Thanks to selecting optimal parameters for each method, these denoised results are all acceptable considering both the preservation of useful signals and removal of random noise. However, we observe that the DSD-based thresholding can get better results in that it leaves practically no coherent useful signal in the noise section (Figure \ref{fig:sign-ddtf-dif}) and its calculated SNR is the highest among all the three approaches. In this example, we used 20 \% coefficients for seislet transform, 16 \% coefficients for DDTF, and 10 \% coefficients for DSD. Figures \ref{fig:sig,sign}, \ref{fig:sign-re,sign-ddtf,sign-ddtf-re}, and \ref{fig:sign-dif,sign-ddtf-dif0,sign-ddtf-dif} have the same scaling in order to make the comparison between different methods fair.  What is interesting about this example is that Figures \ref{fig:sign-dif} and \ref{fig:sign-ddtf-dif} show the fault (as expected), but Figure \ref{fig:sign-ddtf-dif} shows all reflectors nearly equally. This observation is reasonable because DDTF thresholding does not follow the structure of seismic events, thus the denoising process causes equal damage to all events. On the other hand, both seislet thresholding and DSD thresholding include the structure information when constructing the transform, and, therefore, cause smaller damage for events with smoothly changing slope. The damage to the fault regions using traditional denoising approaches is a common problem in seismic data processing. Similar results are also shown by \cite{liuyang2010}. As we can see from this comparison, the denoised result using DSD thresholding can improve the preservation of structural edge information.
%For $f-x$ deconvolution, the prediction filter length is equal to 4.

\inputdir{sig}

\multiplot{2}{sig,sign}{width=0.45\columnwidth}{(a) Clean data. (b) Noisy data.}

\multiplot{3}{sign-re,sign-ddtf,sign-ddtf-re}{width=0.45\columnwidth}{Denoised sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) DSD thresholding. The best results are selected for each method.} 

\multiplot{3}{sign-dif,sign-ddtf-dif0,sign-ddtf-dif}{width=0.45\columnwidth}{Noise sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) DSD thresholding. The best results are selected for each method.} 

\subsection{Field data examples}
\inputdir{field1}

\multiplot{2}{field1,field1n}{width=0.45\columnwidth}{(a) Clean data. (b) Noisy data.}

\multiplot{3}{field1n-re0,field1n-ddtf0,field1n-ddtf-re00}{width=0.45\columnwidth}{Denoised sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) DSD thresholding. The best results are selected for each method.} 

\multiplot{3}{field1n-dif0,field1n-ddtf-dift,field1n-ddtf-dif00}{width=0.45\columnwidth}{Noise sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) DSD thresholding. The best results are selected for each method.} 

For field data tests, we first use a relatively simple 2-D seismic image, shown in Figure \ref{fig:field1}. The simulated noisy data with Gaussian white noise is shown in Figure \ref{fig:field1n}. The denoised results from the three different approaches specified previously are shown in Figure \ref{fig:field1n-re0,field1n-ddtf0,field1n-ddtf-re00}. In this example, we use $7\times7$ patch size for the tight frames and run 30 iterations for updating the tight frames and the coefficients. The corresponding three noise sections are shown in Figure \ref{fig:field1n-dif0,field1n-ddtf-dift,field1n-ddtf-dif00}. From the comparison of denoised results, we conclude that, while all of the three methods can obtain acceptable results, the proposed DSD-based thresholding produces the cleanest image (Figure \ref{fig:field1n-ddtf-re00}) with the least amount of useful energy left in the noise section (Figure \ref{fig:field1n-ddtf-dif00}). The seislet transform based thresholding causes some damage to signal events around time sample 300. The DDTF based thresholding leaves some residual random noise, which comes from the fact that DDTF might automatically learn the behavior of random noise and thus become sensitive to random noise. %Even though $f$-$x$ deconvolution preserves useful energy well, it causes some small artifacts in the middle of the profile. 
In this example, we carefully selected parameters for each method in order to obtain the best performance. Specifically, we use 4 \% coefficients in the seislet domain, 8 \% coefficients in the DDTF domain, and 3 \% coefficients in the DSD domain. %For $f$-$x$ deconvolution, we use 4 as the prediction filter length. 
The comparison of SNRs is shown in the third row in Table \ref{tbl:snrcomp2}. The SNR comparison confirms the previous observation that DSD-based thresholding can obtain the highest SNR.

\inputdir{field2}

\multiplot{2}{field2-f,field2n-f}{width=0.45\columnwidth}{(a) Clean data. (b) Noisy data. The frame boxes are zoomed for detailed comparisons in Figure \ref{fig:field2-f-zoom-a,field2n-f-zoom-a,field2n-re0-f-zoom-a,field2n-ddtf0-f-zoom-a,field2n-ddtf-re00-f-zoom-a}.}

\multiplot{3}{field2n-re0-f,field2n-ddtf0-f,field2n-ddtf-re00-f}{width=0.45\columnwidth}{Denoised sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) DSD thresholding. The best results are selected for each method. The frame boxes are zoomed for detailed comparisons in Figure \ref{fig:field2-f-zoom-a,field2n-f-zoom-a,field2n-re0-f-zoom-a,field2n-ddtf0-f-zoom-a,field2n-ddtf-re00-f-zoom-a}.}

\multiplot{3}{field2n-dif0,field2n-ddtf-dift,field2n-ddtf-dif00}{width=0.45\columnwidth}{Noise sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) DSD thresholding. The best results are selected for each method.} 

\multiplot{5}{field2-f-zoom-a,field2n-f-zoom-a,field2n-re0-f-zoom-a,field2n-ddtf0-f-zoom-a,field2n-ddtf-re00-f-zoom-a}{width=0.45\columnwidth}{Zoomed sections corresponding to the frame boxes shown in Figures \ref{fig:field2-f,field2n-f} and \ref{fig:field2n-re0-f,field2n-ddtf0-f,field2n-ddtf-re00-f}. (a) Clean data. (b) Noisy data. (c) Seislet thresholding. (d) DDTF thresholding. (e) DSD thresholding.} 

Our second field data example is a more complicated case. Figures \ref{fig:field2-f} and \ref{fig:field2n-f} show the clean data and simulated noisy data, respectively. In this example, we use $7\times7$ patch size for the tight frames and run 30 iterations for updating the tight frames and the DSD coefficients. Because of the complex signal structure, seislet transform fails to sparsify the data well. %because of the error in estimating local slope using PWD. Besides, 
On the other hand, because of strong noise, some noise gets treated as useful signal and gets incorporated into the dictionary when constructing DDTF. Therefore, DDTF also fails to obtain good denoising result for this dataset.  The proposed DSD compensates for the weaknesses of both the seislet transform and DDTF and thus achieves a better result. Figure \ref{fig:field2n-re0-f,field2n-ddtf0-f,field2n-ddtf-re00-f} shows the optimal denoised results after using thresholding in the seislet domain with 17 \% coefficients, thresholding in the DDTF domain with 11 \% coefficients, thresholding in the DSD domain with 7 \% coefficients. The corresponding noise sections are shown in Figure \ref{fig:field2n-dif0,field2n-ddtf-dift,field2n-ddtf-dif00}. From observing the denoised results and noise sections, we conclude that thresholding in the DSD domain obtained the cleanest image and with the smallest amount of useful coherent signals lost in the noise sections. The calculated SNRs are shown in the Field 2 row in Table \ref{tbl:snrcomp2}. The SNR comparison confirms the observation that thresholding in DSD domain can get the highest SNR. In this example, we also tried different parameters in order to obtain the best result for each approach. The denoising performance using DSD is noticeably better than the other two approaches, in terms of both SNR measures and visual observation. %Because of the complicated structure, %and inaccurate slope estimation, 
%the seislet domain is no longer sparse, and we need to use 22 \% coefficients to get the best result. However, after learning DSD, we can obtain a much sparser representation for the useful signal in order to perform better thresholding-based denoising. For DSD, we use the least coefficients in the transform domain but get the best denoised result than other sparsity-promoting transforms.

To see the differences more clearly, we zoomed parts of the denoised sections and show them in Figure \ref{fig:field2-f-zoom-a,field2n-f-zoom-a,field2n-re0-f-zoom-a,field2n-ddtf0-f-zoom-a,field2n-ddtf-re00-f-zoom-a}. The zoomed sections correspond to the portions pointed out by the frame boxes in Figures \ref{fig:field2-f,field2n-f} and \ref{fig:field2n-re0-f,field2n-ddtf0-f,field2n-ddtf-re00-f}. We can observe more clearly a better performance of the proposed DSD thresholding in comparison with the other two approaches in that it gets a cleaner image and preserves more of the useful signal. %The difference is not limited to the areas pointed out by the arrows.

Our third field data example is a noisy seismic dataset, shown in Figure \ref{fig:real}. The denoised sections using different denoising approaches are shown in Figure \ref{fig:real-re,real-ddtf-t0,real-fx0,real-sletddtf-re}. In addition to the three thresholding-based approaches, we also demonstrate the denoising performance using the classic $f-x$ deconvolution \cite[]{gulunay1986,galbraith1991,yangkang20141}. 
For $f-x$ deconvolution, we use $50\times50$ windows with 50\% overlap between different local windows. The prediction length is 6 points. These parameters have been tuned for obtaining the best denoising performance. %The detailed algorithm of the $f-x$ deconvolution was given in \cite[]{gulunay1986}.
In this example, we use $7\times7$ patch size for the tight frames and 30 iterations for updating the tight frames and the coefficients in DDTF and DSD.
Correspondingly, Figure \ref{fig:real-dif,real-ddtf-dif,real-fx-dif0,real-sletddtf-dif} shows the noise sections using four approaches. Since for the real case, we do not know the true answer, we are not able to numerically compare the performance of each approach as we did in the previous examples. Instead, we can only judge the performance by observation. The denoised section using $f-x$ deconvolution and its corresponding noise section are shown in Figures \ref{fig:real-fx0} and \ref{fig:real-fx-dif0}, respectively. The denoising results show that the DSD thresholding removes the most noise while preserving the most significant details of the original seismic data. 

\inputdir{real}
\plot{real}{width=0.45\columnwidth}{Real noisy seismic data.} 
\multiplot{4}{real-re,real-ddtf-t0,real-fx0,real-sletddtf-re}{width=0.45\columnwidth}{Denoised sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) $f-x$ deconvolution. (d) DSD thresholding. The best results are selected for each method.}
\multiplot{4}{real-dif,real-ddtf-dif,real-fx-dif0,real-sletddtf-dif}{width=0.45\columnwidth}{Noise sections using different approaches. (a) Seislet thresholding. (b) DDTF thresholding. (c) $f-x$ deconvolution. (d) DSD thresholding. The best results are selected for each method.}


\tabl{snrcomp1}{Comparison of SNR in dB using different approaches for first example (the original SNR is -7.31 dB).}
 {
    \begin{center}
     \begin{tabular}{|c|c|c|c|} 
	  \hline Percentage & Seislet  	& DDTF  & DSD	   \\ 
	  \hline 1 \% 	     & 12.67 & 16.35 &    19.25				\\
	  \hline 2 \% 	     & 13.10 & 17.34 &    \color{red}{\textbf{21.32}}				\\
	  \hline 3 \% 	     & 13.31 & 18.32 &    20.65				\\
	  \hline 4 \% 	     & 15.03 &  \color{red}{\textbf{18.78}}  & 19.74				\\
	  \hline 5 \% 	     & \color{red}{\textbf{15.63}} & 18.56 &    19.32				\\
      \hline 10 \%  	     & 14.84 & 16.32 & 		 15.12    		 			\\ 
      \hline 15 \% 	     & 13.33 	& 12.85 &  11.54\\
      \hline
    \end{tabular} 
   \end{center}
} 

\tabl{snrcomp2}{Comparison of SNR in dB using different approaches. The percentage corresponds to the optimum percentage of coefficients in the transform domain that can obtain the presented SNRs.}
 {
    \begin{center}
     \begin{tabular}{|c|c|c|c|c|}
	  \hline Models	&  Original  & Seislet  	& DDTF 	 & DSD	  \\ 
	  \hline Synthetic 2  	& 20.04  & 33.67 (20\%) & 29.43 (16\%)  & \color{red}{\textbf{37.25}} (10\%) \\
%      \hline Field 1  	    & -4.94  & 6.58  & 15.87 & 15.13 & 19.34 \\ 
      \hline Field 1  	    & -3.015  & 23.12 (4\%)  & 18.04 (8\%) & \color{red}{\textbf{26.34}} (3\%)\\ 
      \hline Field 2   		& -0.94  & 15.54 (17\%) & 18.23 (11\%)  & \color{red}{\textbf{21.97}} (7\%) \\ 
      \hline
    \end{tabular} 
   \end{center}
} 


\section{Discussion}
%Double sparsity is a very general conception. In the reference by \cite{ron2010}, the double sparsity means "sparsity of the coefficients $\mathbf{m}$" and "sparsity of the learning atoms $\mathbf{W}$", i.e., the learning atoms can be sparsely represented by another dictionary or transform, that is to say, one can linearly combine the learning atoms to achieve further sparsity. 
The proposed DSD method attempts to create an optimal combination of the fixed-basis transform and learning-based dictionary, in order to enhance the performance of each individual type of transform and to 
overcome the disadvantages of each individual transform. The seislet transform used in this paper is data-adaptive but depends on the estimation of local slopes of seismic events. An incorrect estimation
of local slopes may make the transform domain less sparse. The DDTF approach used in this paper may not be optimally efficient when large overlap of filtering windows and large number of iterations 
are used, because DDTF is sensitive to small-scale random noise and tends to learn the property of random noise when small window overlap or small number of iterations are used. 
The proposed hybrid framework can relieve the dependency of the seislet transform on slope estimation because the thresholding used for denoising is no longer solely relying on the
sparsity of the seislet domain and instead  is relying on the DSD domain. Because of the initial seislet transform, the transform domain becomes more compactly supported and 
distinguishes signal and noise better. DDTF applied after the seislet transform becomes less sensitive to noise and can achieve better performance when learning the behavior of useful signals. As a result, the 
window overlap and iteration number can both be smaller.

The proposed DSD approach follows the main idea of the multi-scale dictionary learning algorithm proposed by \cite{ophir2011}. The difference between our method and the previous approach is that \cite{ophir2011} use K-SVD in the wavelet domain, while our method uses DDTF in the seislet domain. We use different tools but keep the same strategy. The seislet transform is more suitable than wavelet transform for seismic data. The DDTF is much faster than K-SVD for the dictionary learning stage. \cite{ophir2011} split the data (actually split the subband coefficients) into overlapping patches, and then apply K-SVD on each patch. Analogously, we split the seislet subband data into overlapping patches, and apply DDTF on each patch. There is a control on how much overlapping is used. In general, it is advantageous to apply the maximally overlapping patches (i.e., only shift one column/row between the neighbor patches) for the training. This creates the "richness"/"redundancy" in the training data that generates a shift-invariance for the dictionary. Such shift-invariant wavelets are helpful for denoising, which has been demonstrated in \cite{donoho19951}. In the DDTF, for each patch, the training filer is of tight frame or orthogonal, but the full transform matrix $\mathbf{W}$  is redundant (each column is a training filter on patches).

The double sparsity is more a concept than a specific type of transform. The seislet transform and DDTF used in this paper can be
replaced by other combinations of a fixed-basis transform and a learning-based dictionary to achieve an overall performance which is better than that of each of the
individual transforms. The DSD approach is also not limited to the application to random noise attenuation. Other possible applications include seismic data
interpolation and regularization, simultaneous-source deblending, and sparsity-based seismic inversion.

Both the noisy synthetic data and the first two field data examples shown in this paper we simulated by adding Gaussian white noise. The advantage of using simulated data is that we know the true answer and can numerically calculate SNR in order to better compare and understand the properties of each transform-domain thresholding-based denoising approach. Although the Gaussian white noise is not always appropriate for simulating real random noise in field data, we use it here for simplicity. Changing the Gaussian white noise to colored noise and then performing similar tests is straightforward. 

The fair comparison of different denoising approaches is always a difficult problem. In this paper, we only focus on the coefficient percentage for transform domain thresholding %s and prediction filter length for $f$-$x$ deconvolution. 
and fix other parameters, which mainly include the patch size of the tight frames and the number of iterations to update the tight frames. 

\section{Conclusions}
We have proposed a novel approach to compressing seismic data in order to perform transform-domain thresholding-based random noise attenuation. By considering the benefits of both analytic and learning-based sparsity-promoting transforms, we propose a DSD framework for compensating for the weaknesses of both approaches and sparsifying seismic data for random noise attenuation. We propose two models to construct the DSD framework. One model is based on learning in the data domain, and is called the synthesis-based DSD; the other model is based on learning in the model domain, and is called the analysis-based DSD.  We select the seislet transform as the analytic-basis transform and DDTF as the learning-based sparse dictionary to construct a cascaded DSD framework based on the analysis-based DSD. Synthetic and field data examples with simulated random noise and one field data example with real noise demonstrate a superior performance of the proposed DSD in application to random noise attenuation of seismic reflection data. 

\section{Acknowledgments}
We would like to thank Mauricio Sacchi, Jeffrey Shragge, Ian Moore, David Halliday, and three anonymous reviewers for constructive suggestions. The paper was prepared in the Madagascar open-source platform \cite[]{mada2013}, and all the examples are reproducible. This work is partially supported by the Natural Science Fund of China(grant NOs.:91330108, 41374121, and 61327013), the fundamental Research Funds for the Central Universities (grant NO.: HIT.BRETIV.201314), and the Program for New Century Excellent Talents in University (grant NO.: NCET-11-0804), and the Texas Consortium for Computational Seismology (TCCS).


\bibliographystyle{seg}
\bibliography{ddtfseis}
